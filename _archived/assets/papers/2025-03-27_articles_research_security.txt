{
    "SoK: Decoding the Enigma of Encrypted Network Traffic Classifiers": {
        "title": "SoK: Decoding the Enigma of Encrypted Network Traffic Classifiers",
        "link": "https://arxiv.org/abs/2503.20093",
        "summary": "arXiv:2503.20093v1 Announce Type: new \nAbstract: The adoption of modern encryption protocols such as TLS 1.3 has significantly challenged traditional network traffic classification (NTC) methods. As a consequence, researchers are increasingly turning to machine learning (ML) approaches to overcome these obstacles. In this paper, we comprehensively analyze ML-based NTC studies, developing a taxonomy of their design choices, benchmarking suites, and prevalent assumptions impacting classifier performance. Through this systematization, we demonstrate widespread reliance on outdated datasets, oversights in design choices, and the consequences of unsubstantiated assumptions. Our evaluation reveals that the majority of proposed encrypted traffic classifiers have mistakenly utilized unencrypted traffic due to the use of legacy datasets. Furthermore, by conducting 348 feature occlusion experiments on state-of-the-art classifiers, we show how oversights in NTC design choices lead to overfitting, and validate or refute prevailing assumptions with empirical evidence. By highlighting lessons learned, we offer strategic insights, identify emerging research directions, and recommend best practices to support the development of real-world applicable NTC methodologies.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Software Vulnerability Analysis Across Programming Language and Program Representation Landscapes: A Survey": {
        "title": "Software Vulnerability Analysis Across Programming Language and Program Representation Landscapes: A Survey",
        "link": "https://arxiv.org/abs/2503.20244",
        "summary": "arXiv:2503.20244v1 Announce Type: new \nAbstract: Modern software systems are developed in diverse programming languages and often harbor critical vulnerabilities that attackers can exploit to compromise security. These vulnerabilities have been actively targeted in real-world attacks, causing substantial harm to users and cyberinfrastructure. Since many of these flaws originate from the code itself, a variety of techniques have been proposed to detect and mitigate them prior to software deployment. However, a comprehensive comparative study that spans different programming languages, program representations, bug types, and analysis techniques is still lacking. As a result, the relationships among programming languages, abstraction levels, vulnerability types, and detection approaches remain fragmented, and the limitations and research gaps across the landscape are not clearly understood. This article aims to bridge that gap by systematically examining widely used programming languages, levels of program representation, categories of vulnerabilities, and mainstream detection techniques. The survey provides a detailed understanding of current practices in vulnerability discovery, highlighting their strengths, limitations, and distinguishing characteristics. Furthermore, it identifies persistent challenges and outlines promising directions for future research in the field of software security.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "How Secure is Forgetting? Linking Machine Unlearning to Machine Learning Attacks": {
        "title": "How Secure is Forgetting? Linking Machine Unlearning to Machine Learning Attacks",
        "link": "https://arxiv.org/abs/2503.20257",
        "summary": "arXiv:2503.20257v1 Announce Type: new \nAbstract: As Machine Learning (ML) evolves, the complexity and sophistication of security threats against this paradigm continue to grow as well, threatening data privacy and model integrity. In response, Machine Unlearning (MU) is a recent technology that aims to remove the influence of specific data from a trained model, enabling compliance with privacy regulations and user requests. This can be done for privacy compliance (e.g., GDPR's right to be forgotten) or model refinement. However, the intersection between classical threats in ML and MU remains largely unexplored. In this Systematization of Knowledge (SoK), we provide a structured analysis of security threats in ML and their implications for MU. We analyze four major attack classes, namely, Backdoor Attacks, Membership Inference Attacks (MIA), Adversarial Attacks, and Inversion Attacks, we investigate their impact on MU and propose a novel classification based on how they are usually used in this context. Finally, we identify open challenges, including ethical considerations, and explore promising future research directions, paving the way for future research in secure and privacy-preserving Machine Unlearning.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Are We There Yet? Unraveling the State-of-the-Art Graph Network Intrusion Detection Systems": {
        "title": "Are We There Yet? Unraveling the State-of-the-Art Graph Network Intrusion Detection Systems",
        "link": "https://arxiv.org/abs/2503.20281",
        "summary": "arXiv:2503.20281v1 Announce Type: new \nAbstract: Network Intrusion Detection Systems (NIDS) are vital for ensuring enterprise security. Recently, Graph-based NIDS (GIDS) have attracted considerable attention because of their capability to effectively capture the complex relationships within the graph structures of data communications. Despite their promise, the reproducibility and replicability of these GIDS remain largely unexplored, posing challenges for developing reliable and robust detection systems. This study bridges this gap by designing a systematic approach to evaluate state-of-the-art GIDS, which includes critically assessing, extending, and clarifying the findings of these systems. We further assess the robustness of GIDS under adversarial attacks. Evaluations were conducted on three public datasets as well as a newly collected large-scale enterprise dataset. Our findings reveal significant performance discrepancies, highlighting challenges related to dataset scale, model inputs, and implementation settings. We demonstrate difficulties in reproducing and replicating results, particularly concerning false positive rates and robustness against adversarial attacks. This work provides valuable insights and recommendations for future research, emphasizing the importance of rigorous reproduction and replication studies in developing robust and generalizable GIDS solutions.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "UnReference: analysis of the effect of spoofing on RTK reference stations for connected rovers": {
        "title": "UnReference: analysis of the effect of spoofing on RTK reference stations for connected rovers",
        "link": "https://arxiv.org/abs/2503.20364",
        "summary": "arXiv:2503.20364v1 Announce Type: new \nAbstract: Global Navigation Satellite Systems (GNSS) provide standalone precise navigation for a wide gamut of applications. Nevertheless, applications or systems such as unmanned vehicles (aerial or ground vehicles and surface vessels) generally require a much higher level of accuracy than those provided by standalone receivers. The most effective and economical way of achieving centimeter-level accuracy is to rely on corrections provided by fixed \\emph{reference station} receivers to improve the satellite ranging measurements. Differential GNSS (DGNSS) and Real Time Kinematics (RTK) provide centimeter-level accuracy by distributing online correction streams to connected nearby mobile receivers typically termed \\emph{rovers}. However, due to their static nature, reference stations are prime targets for GNSS attacks, both simplistic jamming and advanced spoofing, with different levels of adversarial control and complexity. Jamming the reference station would deny corrections and thus accuracy to the rovers. Spoofing the reference station would force it to distribute misleading corrections. As a result, all connected rovers using those corrections will be equally influenced by the adversary independently of their actual trajectory. We evaluate a battery of tests generated with an RF simulator to test the robustness of a common DGNSS/RTK processing library and receivers. We test both jamming and synchronized spoofing to demonstrate that adversarial action on the rover using reference spoofing is both effective and convenient from an adversarial perspective. Additionally, we discuss possible strategies based on existing countermeasures (self-validation of the PNT solution and monitoring of own clock drift) that the rover and the reference station can adopt to avoid using or distributing bogus corrections.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "ARGO-SLSA: Software Supply Chain Security in Argo Workflows": {
        "title": "ARGO-SLSA: Software Supply Chain Security in Argo Workflows",
        "link": "https://arxiv.org/abs/2503.20079",
        "summary": "arXiv:2503.20079v1 Announce Type: cross \nAbstract: Distributed systems widely adopt microservice architecture to handle growing complexity and scale. This approach breaks applications into independent, loosely coupled services. Kubernetes has become the de facto standard for managing microservices, and automating complex, multi-step workflows is a common requirement in Kubernetes. Argo Workflows is a Kubernetes-native engine for managing these workflows in an automated fashion. These workflows generate artifacts such as executables, logs, container images, and packages, which often require proper management through software supply chain security. However, Argo Workflows does not include built-in functionality for frameworks like Supply-chain Levels for Software Artifacts (SLSA), which is essential for ensuring artifact integrity, traceability, and security. This gap compels practitioners to rely on external tools to meet software supply chain security standards. In response, this paper proposes a Kubernetes-native controller built on top of existing open-source Argo Workflows to enhance artifact security. By generating cryptographic signing and provenance attestations, the controller enables Argo Workflows to comply with SLSA standards. We demonstrate that implementations can provide such cryptographic signing and provenance attestations for artifacts produced by the controller, allowing software artifacts built with Argo Workflows to adhere to SLSA requirements. The proposed validation model evaluates the proof of concept of the controller, including its ability to reconcile workflows, detect pods associated with workflow nodes, operate without disrupting existing operations, enforce integrity, and monitor software artifacts.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "A Blockchain-based Quantum Binary Voting for Decentralized IoT Towards Industry 5.0": {
        "title": "A Blockchain-based Quantum Binary Voting for Decentralized IoT Towards Industry 5.0",
        "link": "https://arxiv.org/abs/2503.20247",
        "summary": "arXiv:2503.20247v1 Announce Type: cross \nAbstract: Industry 5.0 depends on intelligence, automation, and hyperconnectivity operations for effective and sustainable human-machine collaboration. Pivotal technologies like the Internet of Things (IoT) enable this by facilitating connectivity and data-driven decision-making between cyber-physical devices. As IoT devices are prone to cyberattacks, they can use blockchain to improve transparency in the network and prevent data tampering. However, in some cases, even blockchain networks are vulnerable to Sybil and 51% attacks. This has motivated the development of quantum blockchains that are more resilient to such attacks as they leverage post-quantum cryptographic protocols and secure quantum communication channels. In this work, we develop a quantum binary voting algorithm for the IoT-quantum blockchain frameworks that enables inter-connected devices to reach a consensus on the validity of transactions, even in the presence of potential faults or malicious actors. The correctness of the voting protocol is provided in detail, and the results show that it guarantees the achievement of a consensus securely against all kinds of significant external and internal attacks concerning quantum bit commitment, quantum blockchain, and quantum Byzantine agreement. We also provide an implementation of the voting algorithm with the quantum circuits simulated on the IBM Quantum platform and Simulaqron library.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "sudo rm -rf agentic_security": {
        "title": "sudo rm -rf agentic_security",
        "link": "https://arxiv.org/abs/2503.20279",
        "summary": "arXiv:2503.20279v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal trained safeguards in commercial computer-use agents, such as Claude Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with no refinement), and up to 41% (by its iterative refinement) in Claude Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks": {
        "title": "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks",
        "link": "https://arxiv.org/abs/2503.20310",
        "summary": "arXiv:2503.20310v1 Announce Type: cross \nAbstract: Adversarial attacks in black-box settings are highly practical, with transfer-based attacks being the most effective at generating adversarial examples (AEs) that transfer from surrogate models to unseen target models. However, their performance significantly degrades when transferring across heterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers (ViTs) -- due to fundamental architectural differences. To address this, we propose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method that enhances adversarial transferability across diverse architectures. FPA introduces a novel feature permutation (FP) operation, which rearranges pixel values in selected feature maps to simulate long-range dependencies, effectively making CNNs behave more like ViTs and MLPs. This enhances feature diversity and improves transferability both across heterogeneous architectures and within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art architectures show that FPA achieves maximum absolute gains in attack success rates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming existing black-box attacks. Additionally, FPA is highly generalizable and can seamlessly integrate with other transfer-based attacks to further boost their performance. Our findings establish FPA as a robust, efficient, and computationally lightweight strategy for enhancing adversarial transferability across heterogeneous architectures.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Power Networks SCADA Communication Cybersecurity, A Qiskit Implementation": {
        "title": "Power Networks SCADA Communication Cybersecurity, A Qiskit Implementation",
        "link": "https://arxiv.org/abs/2503.20365",
        "summary": "arXiv:2503.20365v1 Announce Type: cross \nAbstract: The cyber-physical system of electricity power networks utilizes supervisory control and data acquisition systems (SCADA), which are inherently vulnerable to cyber threats if usually connected with the internet technology (IT). Power system operations are conducted through communication systems that are mapped to standards, protocols, ports, and addresses. Real-time situational awareness is a standard term with implications and applications in both power systems and cybersecurity. In the plausible quantum world (Q-world), conventional approaches will likely face new challenges. The unique art of transmitting a quantum state from one place, Alice, to another, Bob, is known as quantum communication. Quantum communication for SCADA communication in a plausible quantum era thus obviously entails wired communication through optical fiber networks complying with the typical cybersecurity criteria of confidentiality, integrity, and availability for classical internet technology unless a quantum internet (qinternet) transpires practically. When combined with the reverse order of AIC for operational technology, the cybersecurity criteria for power networks' critical infrastructure drill down to more specific sub-areas. Unlike other communication modes, such as information technology (IT) in broadband internet connections, SCADA for power networks, one of the critical infrastructures, is intricately intertwined with operations technology (OT), which significantly increases complexity. Though it is desirable to have a barrier called a demilitarized zone (DMZ), some overlap is inevitable. This paper highlights the opportunities and challenges in securing SCADA communication in the plausible quantum computing and communication regime, along with a corresponding integrated Qiskit implementation for possible future framework development.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Certified randomness using a trapped-ion quantum processor": {
        "title": "Certified randomness using a trapped-ion quantum processor",
        "link": "https://arxiv.org/abs/2503.20498",
        "summary": "arXiv:2503.20498v1 Announce Type: cross \nAbstract: While quantum computers have the potential to perform a wide range of practically important tasks beyond the capabilities of classical computers, realizing this potential remains a challenge. One such task is to use an untrusted remote device to generate random bits that can be certified to contain a certain amount of entropy. Certified randomness has many applications but is fundamentally impossible to achieve solely by classical computation. In this work, we demonstrate the generation of certifiably random bits using the 56-qubit Quantinuum H2-1 trapped-ion quantum computer accessed over the internet. Our protocol leverages the classical hardness of recent random circuit sampling demonstrations: a client generates quantum \"challenge\" circuits using a small randomness seed, sends them to an untrusted quantum server to execute, and verifies the server's results. We analyze the security of our protocol against a restricted class of realistic near-term adversaries. Using classical verification with measured combined sustained performance of $1.1\\times10^{18}$ floating-point operations per second across multiple supercomputers, we certify $71,313$ bits of entropy under this restricted adversary and additional assumptions. Our results demonstrate a step towards the practical applicability of today's quantum computers.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Defending against Backdoor Attack on Deep Neural Networks": {
        "title": "Defending against Backdoor Attack on Deep Neural Networks",
        "link": "https://arxiv.org/abs/2002.12162",
        "summary": "arXiv:2002.12162v3 Announce Type: replace \nAbstract: Although deep neural networks (DNNs) have achieved a great success in various computer vision tasks, it is recently found that they are vulnerable to adversarial attacks. In this paper, we focus on the so-called \\textit{backdoor attack}, which injects a backdoor trigger to a small portion of training data (also known as data poisoning) such that the trained DNN induces misclassification while facing examples with this trigger. To be specific, we carefully study the effect of both real and synthetic backdoor attacks on the internal response of vanilla and backdoored DNNs through the lens of Gard-CAM. Moreover, we show that the backdoor attack induces a significant bias in neuron activation in terms of the $\\ell_\\infty$ norm of an activation map compared to its $\\ell_1$ and $\\ell_2$ norm. Spurred by our results, we propose the \\textit{$\\ell_\\infty$-based neuron pruning} to remove the backdoor from the backdoored DNN. Experiments show that our method could effectively decrease the attack success rate, and also hold a high classification accuracy for clean images.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "UPPRESSO: Untraceable and Unlinkable Privacy-PREserving Single Sign-On Services": {
        "title": "UPPRESSO: Untraceable and Unlinkable Privacy-PREserving Single Sign-On Services",
        "link": "https://arxiv.org/abs/2110.10396",
        "summary": "arXiv:2110.10396v3 Announce Type: replace \nAbstract: Single sign-on (SSO) allows a user to maintain only the credential for an identity provider (IdP) to log into multiple relying parties (RPs). However, SSO introduces privacy threats, as (a) a curious IdP could track a user's all visits to RPs, and (b) colluding RPs could learn a user's online profile by linking her identities across these RPs. This paper presents a privacypreserving SSO scheme, called UPPRESSO, to protect an honest user's online profile against (a) an honest-but-curious IdP and (b) malicious RPs colluding with other users. UPPRESSO proposes an identity-transformation approach to generate untraceable ephemeral pseudo-identities for an RP and a user from which the target RP derives a permanent account for the user, while the transformations also provide unlinkability. This approach protects the identities of the user and the target RPs in a login flow, while working compatibly with widely-deployed SSO protocols and providing services accessed from a commercial-off-the-shelf browser without plug-ins or extensions. We built a prototype of UPPRESSO on top of MITREid Connect, an open-source SSO system. The extensive evaluations show that it fulfills the security and privacy requirements of SSO with reasonable overheads.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "SE#PCFG: Semantically Enhanced PCFG for Password Analysis and Cracking": {
        "title": "SE#PCFG: Semantically Enhanced PCFG for Password Analysis and Cracking",
        "link": "https://arxiv.org/abs/2306.06824",
        "summary": "arXiv:2306.06824v2 Announce Type: replace \nAbstract: Much research has been done on user-generated textual passwords. Surprisingly, semantic information in such passwords remain under-investigated, with passwords created by English- and/or Chinese-speaking users being more studied with limited semantics. This paper fills this gap by proposing a general framework based on semantically enhanced PCFG (probabilistic context-free grammars) named SE#PCFG. It allowed us to consider 43 types of semantic information, the richest set considered so far, for password analysis. Applying SE#PCFG to 17 large leaked password databases of user speaking four languages (English, Chinese, German and French), we demonstrate its usefulness and report a wide range of new insights about password semantics at different levels such as cross-website password correlations. Furthermore, based on SE#PCFG and a new systematic smoothing method, we proposed the Semantically Enhanced Password Cracking Architecture (SEPCA), and compared its performance against three SOTA (state-of-the-art) benchmarks in terms of the password coverage rate: two other PCFG variants and neural network. Our experimental results showed that SEPCA outperformed all the three benchmarks consistently and significantly across 52 test cases, by up to 21.53%, 52.55% and 7.86%, respectively, at the user-level (with duplicate passwords). At the level of unique passwords, SEPCA also beats the three counterparts by up to 43.83%, 94.11% and 11.16%, respectively.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Hiding Functions within Functions: Steganography by Implicit Neural Representations": {
        "title": "Hiding Functions within Functions: Steganography by Implicit Neural Representations",
        "link": "https://arxiv.org/abs/2312.04743",
        "summary": "arXiv:2312.04743v2 Announce Type: replace \nAbstract: Deep steganography utilizes the powerful capabilities of deep neural networks to embed and extract messages, but its reliance on an additional message extractor limits its practical use due to the added suspicion it can raise from steganalyzers. To address this problem, we propose StegaINR, which utilizes Implicit Neural Representation (INR) to implement steganography. StegaINR embeds a secret function into a stego function, which serves as both the message extractor and the stego media for secure transmission on a public channel. Recipients need only use a shared key to recover the secret function from the stego function, allowing them to obtain the secret message. Our approach makes use of continuous functions, enabling it to handle various types of messages. To our knowledge, this is the first work to introduce INR into steganography. We performed evaluations on image and climate data to test our method in different deployment contexts.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "MERGE: Matching Electronic Results with Genuine Evidence for verifiable voting in person at remote locations": {
        "title": "MERGE: Matching Electronic Results with Genuine Evidence for verifiable voting in person at remote locations",
        "link": "https://arxiv.org/abs/2410.06705",
        "summary": "arXiv:2410.06705v4 Announce Type: replace \nAbstract: Overseas military personnel often face significant challenges in participating in elections due to the slow pace of traditional mail systems, which can result in ballots missing crucial deadlines. While internet-based voting offers a faster alternative, it introduces serious risks to the integrity and privacy of the voting process. We introduce the MERGE protocol to address these issues by combining the speed of electronic ballot delivery with the reliability of paper returns. This protocol allows voters to submit an electronic record of their vote quickly while simultaneously mailing a paper ballot for verification. The electronic record can be used for preliminary results, but the paper ballot is used in a Risk Limiting Audit (RLA) if received in time, ensuring the integrity of the election. This approach extends the time window for ballot arrival without undermining the security and accuracy of the vote count.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "PAPILLON: Privacy Preservation from Internet-based and Local Language Model Ensembles": {
        "title": "PAPILLON: Privacy Preservation from Internet-based and Local Language Model Ensembles",
        "link": "https://arxiv.org/abs/2410.17127",
        "summary": "arXiv:2410.17127v3 Announce Type: replace \nAbstract: Users can divulge sensitive information to proprietary LLM providers, raising significant privacy concerns. While open-source models, hosted locally on the user's machine, alleviate some concerns, models that users can host locally are often less capable than proprietary frontier models. Toward preserving user privacy while retaining the best quality, we propose Privacy-Conscious Delegation, a novel task for chaining API-based and local models. We utilize recent public collections of user-LLM interactions to construct a natural benchmark called PUPA, which contains personally identifiable information (PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM pipeline that uses prompt optimization to address a simpler version of our task. Our best pipeline maintains high response quality for 85.5% of user queries while restricting privacy leakage to only 7.5%. We still leave a large margin to the generation quality of proprietary LLMs for future work. Our data and code is available at https://github.com/siyan-sylvia-li/PAPILLON.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "The importance of the clustering model to detect new types of intrusion in data traffic": {
        "title": "The importance of the clustering model to detect new types of intrusion in data traffic",
        "link": "https://arxiv.org/abs/2411.14550",
        "summary": "arXiv:2411.14550v2 Announce Type: replace \nAbstract: In the current digital age, the volume of data generated by various cyber activities has become enormous and is constantly increasing. The data may contain valuable insights that can be harnessed to improve cyber security measures. However, much of this data is unclassified and qualitative, which poses significant challenges to traditional analysis methods. Clustering facilitates the identification of hidden patterns and structures in data through grouping similar data points, which makes it simpler to identify and address threats. Clustering can be defined as a data mining (DM) approach, which uses similarity calculations for dividing a data set into several categories. Hierarchical, density-based, along with partitioning clustering algorithms are typical. The presented work use K-means algorithm, which is a popular clustering technique. Utilizing K-means algorithm, we worked with two different types of data: first, we gathered data with the use of XG-boost algorithm following completing the aggregation with K-means algorithm. Data was gathered utilizing Kali Linux environment, cicflowmeter traffic, and Putty Software tools with the use of diverse and simple attacks. The concept could assist in identifying new attack types, which are distinct from the known attacks, and labeling them based on the characteristics they will exhibit, as the dynamic nature regarding cyber threats means that new attack types often emerge, for which labeled data might not yet exist. The model counted the attacks and assigned numbers to each one of them. Secondly, We tried the same work on the ready data inside the Kaggle repository called (Intrusion Detection in Internet of Things Network), and the clustering model worked well and detected the number of attacks correctly as shown in the results section.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models": {
        "title": "Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models",
        "link": "https://arxiv.org/abs/2412.03283",
        "summary": "arXiv:2412.03283v2 Announce Type: replace \nAbstract: Integrating watermarking into the generation process of latent diffusion models (LDMs) simplifies detection and attribution of generated content. Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel class of watermarking techniques that are easy to implement and highly robust against various perturbations. However, our work demonstrates a fundamental security vulnerability of semantic watermarks. We show that attackers can leverage unrelated models, even with different latent spaces and architectures (UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically, we design two watermark forgery attacks. The first imprints a targeted watermark into real images by manipulating the latent representation of an arbitrary image in an unrelated LDM to get closer to the latent representation of a watermarked image. We also show that this technique can be used for watermark removal. The second attack generates new images with the target watermark by inverting a watermarked image and re-generating it with an arbitrary prompt. Both attacks just need a single reference image with the target watermark. Overall, our findings question the applicability of semantic watermarks by revealing that attackers can easily forge or remove these watermarks under realistic conditions.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "A Survey of Secure Semantic Communications": {
        "title": "A Survey of Secure Semantic Communications",
        "link": "https://arxiv.org/abs/2501.00842",
        "summary": "arXiv:2501.00842v2 Announce Type: replace \nAbstract: Semantic communication (SemCom) is regarded as a promising and revolutionary technology in 6G, aiming to transcend the constraints of ``Shannon's trap\" by filtering out redundant information and extracting the core of effective data. Compared to traditional communication paradigms, SemCom offers several notable advantages, such as reducing the burden on data transmission, enhancing network management efficiency, and optimizing resource allocation. Numerous researchers have extensively explored SemCom from various perspectives, including network architecture, theoretical analysis, potential technologies, and future applications. However, as SemCom continues to evolve, a multitude of security and privacy concerns have arisen, posing threats to the confidentiality, integrity, and availability of SemCom systems. This paper presents a comprehensive survey of the technologies that can be utilized to secure SemCom. Firstly, we elaborate on the entire life cycle of SemCom, which includes the model training, model transfer, and semantic information transmission phases. Then, we identify the security and privacy issues that emerge during these three stages. Furthermore, we summarize the techniques available to mitigate these security and privacy threats, including data cleaning, robust learning, defensive strategies against backdoor attacks, adversarial training, differential privacy, cryptography, blockchain technology, model compression, and physical-layer security. Lastly, this paper outlines future research directions to guide researchers in related fields.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Joint Task Offloading and User Scheduling in 5G MEC under Jamming Attacks": {
        "title": "Joint Task Offloading and User Scheduling in 5G MEC under Jamming Attacks",
        "link": "https://arxiv.org/abs/2501.13227",
        "summary": "arXiv:2501.13227v2 Announce Type: replace \nAbstract: In this paper, we propose a novel joint task offloading and user scheduling (JTO-US) framework for 5G mobile edge computing (MEC) systems under security threats from jamming attacks. The goal is to minimize the delay and the ratio of dropped tasks, taking into account both communication and computation delays. The system model includes a 5G network equipped with MEC servers and an adversarial on-off jammer that disrupts communication. The proposed framework optimally schedules tasks and users to minimize the impact of jamming while ensuring that high-priority tasks are processed efficiently. Genetic algorithm (GA) is used to solve the optimization problem, and the results are compared with benchmark methods such as GA without considering jamming effect, Shortest Job First (SJF), and Shortest Deadline First (SDF). The simulation results demonstrate that the proposed JTO-US framework achieves the lowest drop ratio and effectively manages priority tasks, outperforming existing methods. Particularly, when the jamming probability is 0.8, the proposed framework mitigates the jammer's impact by reducing the drop ratio to 63%, compared to 89% achieved by the next best method.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Intelligent Code Embedding Framework for High-Precision Ransomware Detection via Multimodal Execution Path Analysis": {
        "title": "Intelligent Code Embedding Framework for High-Precision Ransomware Detection via Multimodal Execution Path Analysis",
        "link": "https://arxiv.org/abs/2501.15836",
        "summary": "arXiv:2501.15836v2 Announce Type: replace \nAbstract: Modern threat landscapes continue to evolve with increasing sophistication, challenging traditional detection methodologies and necessitating innovative solutions capable of addressing complex adversarial tactics. A novel framework was developed to identify ransomware activity through multimodal execution path analysis, integrating high-dimensional embeddings and dynamic heuristic derivation mechanisms to capture behavioral patterns across diverse attack variants. The approach demonstrated high adaptability, effectively mitigating obfuscation strategies and polymorphic characteristics often employed by ransomware families to evade detection. Comprehensive experimental evaluations revealed significant advancements in precision, recall, and accuracy metrics compared to baseline techniques, particularly under conditions of variable encryption speeds and obfuscated execution flows. The framework achieved scalable and computationally efficient performance, ensuring robust applicability across a range of system configurations, from resource-constrained environments to high-performance infrastructures. Notable findings included reduced false positive rates and enhanced detection latency, even for ransomware families employing sophisticated encryption mechanisms. The modular design allowed seamless integration of additional modalities, enabling extensibility and future-proofing against emerging threat vectors. Quantitative analyses further highlighted the system's energy efficiency, emphasizing its practicality for deployment in environments with stringent operational constraints. The results underline the importance of integrating advanced computational techniques and dynamic adaptability to safeguard digital ecosystems from increasingly complex threats.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Decentralized Entropy-Driven Ransomware Detection Using Autonomous Neural Graph Embeddings": {
        "title": "Decentralized Entropy-Driven Ransomware Detection Using Autonomous Neural Graph Embeddings",
        "link": "https://arxiv.org/abs/2502.07498",
        "summary": "arXiv:2502.07498v2 Announce Type: replace \nAbstract: The increasing sophistication of cyber threats has necessitated the development of advanced detection mechanisms capable of identifying and mitigating ransomware attacks with high precision and efficiency. A novel framework, termed Decentralized Entropy-Driven Detection (DED), is introduced, leveraging autonomous neural graph embeddings and entropy-based anomaly scoring to address the limitations of traditional methods. The framework operates on a distributed network of nodes, eliminating single points of failure and enhancing resilience against targeted attacks. Experimental results demonstrate its ability to achieve detection accuracy exceeding 95\\%, with false positive rates maintained below 2\\% across diverse ransomware variants. The integration of graph-based modeling and machine learning techniques enables the framework to capture complex system interactions, facilitating the identification of subtle anomalies indicative of ransomware activity. Comparative analysis against existing methods highlights its superior performance in terms of detection rates and computational efficiency. Case studies further validate its effectiveness in real-world scenarios, showcasing its ability to detect and mitigate ransomware attacks within minutes of their initiation. The proposed framework represents a significant step forward in cybersecurity, offering a scalable and adaptive solution to the growing challenge of ransomware detection.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Decentralized Entropy-Based Ransomware Detection Using Autonomous Feature Resonance": {
        "title": "Decentralized Entropy-Based Ransomware Detection Using Autonomous Feature Resonance",
        "link": "https://arxiv.org/abs/2502.09833",
        "summary": "arXiv:2502.09833v2 Announce Type: replace \nAbstract: The increasing sophistication of cyber threats has necessitated the development of advanced detection mechanisms capable of identifying malicious activities with high precision and efficiency. A novel approach, termed Autonomous Feature Resonance, is introduced to address the limitations of traditional ransomware detection methods through the analysis of entropy-based feature interactions within system processes. The proposed method achieves an overall detection accuracy of 97.3\\%, with false positive and false negative rates of 1.8\\% and 2.1\\%, respectively, outperforming existing techniques such as signature-based detection and behavioral analysis. Its decentralized architecture enables local processing of data, reducing latency and improving scalability, while a self-learning mechanism ensures continuous adaptation to emerging threats. Experimental results demonstrate consistent performance across diverse ransomware families, including LockBit 3.0, BlackCat, and Royal, with low detection latency and efficient resource utilization. The method's reliance on entropy as a distinguishing feature provides robustness against obfuscation techniques, making it suitable for real-time deployment in high-throughput environments. These findings highlight the potential of entropy-based approaches to enhance cybersecurity frameworks, offering a scalable and adaptive solution for modern ransomware detection challenges.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "A Computational Model for Ransomware Detection Using Cross-Domain Entropy Signatures": {
        "title": "A Computational Model for Ransomware Detection Using Cross-Domain Entropy Signatures",
        "link": "https://arxiv.org/abs/2502.10711",
        "summary": "arXiv:2502.10711v2 Announce Type: replace \nAbstract: Detecting encryption-driven cyber threats remains a large challenge due to the evolving techniques employed to evade traditional detection mechanisms. An entropy-based computational framework was introduced to analyze multi-domain system variations, enabling the identification of malicious encryption behaviors through entropy deviations. By integrating entropy patterns across file operations, memory allocations, and network transmissions, a detection methodology was developed to differentiate between benign and ransomware-induced entropy shifts. A mathematical model was formulated to quantify entropy dynamics, incorporating time-dependent variations and weighted domain contributions to enhance anomaly detection. Experimental evaluations demonstrated that the proposed approach achieved high accuracy across diverse ransomware families while maintaining low false positive rates. Computational efficiency analysis indicated minimal processing overhead, suggesting feasibility for real-time implementation in security-sensitive environments. The study highlighted entropy fluctuations as a useful indicator for identifying malicious encryption processes, reinforcing entropy-driven methodologies as a viable component of cybersecurity strategies.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Knowledge Transfer from LLMs to Provenance Analysis: A Semantic-Augmented Method for APT Detection": {
        "title": "Knowledge Transfer from LLMs to Provenance Analysis: A Semantic-Augmented Method for APT Detection",
        "link": "https://arxiv.org/abs/2503.18316",
        "summary": "arXiv:2503.18316v2 Announce Type: replace \nAbstract: Advanced Persistent Threats (APTs) have caused significant losses across a wide range of sectors, including the theft of sensitive data and harm to system integrity. As attack techniques grow increasingly sophisticated and stealthy, the arms race between cyber defenders and attackers continues to intensify. The revolutionary impact of Large Language Models (LLMs) has opened up numerous opportunities in various fields, including cybersecurity. An intriguing question arises: can the extensive knowledge embedded in LLMs be harnessed for provenance analysis and play a positive role in identifying previously unknown malicious events? To seek a deeper understanding of this issue, we propose a new strategy for taking advantage of LLMs in provenance-based threat detection. In our design, the state-of-the-art LLM offers additional details in provenance data interpretation, leveraging their knowledge of system calls, software identity, and high-level understanding of application execution context. The advanced contextualized embedding capability is further utilized to capture the rich semantics of event descriptions. We comprehensively examine the quality of the resulting embeddings, and it turns out that they offer promising avenues. Subsequently, machine learning models built upon these embeddings demonstrated outstanding performance on real-world data. In our evaluation, supervised threat detection achieves a precision of 99.0%, and semi-supervised anomaly detection attains a precision of 96.9%.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Mixture of Robust Experts (MoRE):A Robust Denoising Method towards multiple perturbations": {
        "title": "Mixture of Robust Experts (MoRE):A Robust Denoising Method towards multiple perturbations",
        "link": "https://arxiv.org/abs/2104.10586",
        "summary": "arXiv:2104.10586v5 Announce Type: replace-cross \nAbstract: To tackle the susceptibility of deep neural networks to adversarial examples, the adversarial training has been proposed which provides a notion of security through an inner maximization problem presenting the first-order adversaries embedded within the outer minimization of the training loss. To generalize the adversarial robustness over different perturbation types, the adversarial training method has been augmented with the improved inner maximization presenting a union of multiple perturbations e.g., various $\\ell_p$ norm-bounded perturbations. However, the improved inner maximization only enjoys limited flexibility in terms of the allowable perturbation types. In this work, through a gating mechanism, we assemble a set of expert networks, each one either adversarially trained to deal with a particular perturbation type or normally trained for boosting accuracy on clean data. The gating module assigns weights dynamically to each expert to achieve superior accuracy under various data types e.g., adversarial examples, adverse weather perturbations, and clean input. In order to deal with the obfuscated gradients issue, the training of the gating module is conducted together with fine-tuning of the last fully connected layers of expert networks through adversarial training approach. Using extensive experiments, we show that our Mixture of Robust Experts (MoRE) approach enables a flexible integration of a broad range of robust experts with superior performance.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Consensus Capacity of Noisy Broadcast Channels": {
        "title": "Consensus Capacity of Noisy Broadcast Channels",
        "link": "https://arxiv.org/abs/2205.06073",
        "summary": "arXiv:2205.06073v4 Announce Type: replace-cross \nAbstract: We study communication with consensus over a broadcast channel - the receivers reliably decode the sender's message when the sender is honest, and their decoder outputs agree even if the sender acts maliciously. We characterize the broadcast channels which permit this byzantine consensus and determine their capacity. We show that communication with consensus is possible only when the broadcast channel has embedded in it a natural ''common channel'' whose output both receivers can unambiguously determine from their own channel outputs. Interestingly, in general, the consensus capacity may be larger than the point-to-point capacity of the common channel, i.e., while decoding, the receivers may make use of parts of their output signals on which they may not have consensus provided there are some parts (namely, the common channel output) on which they can agree.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach": {
        "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach",
        "link": "https://arxiv.org/abs/2401.08351",
        "summary": "arXiv:2401.08351v2 Announce Type: replace-cross \nAbstract: Federated Learning (FL) aims to infer a shared model from private and decentralized data stored by multiple clients. Personalized FL (PFL) enhances the model's fit for each client by adapting the global model to the clients. A significant level of personalization is required for highly heterogeneous clients but can be challenging to achieve, especially when clients' datasets are small. To address this issue, we introduce the PAC-PFL framework for PFL of probabilistic models. PAC-PFL infers a shared hyper-posterior and treats each client's posterior inference as the personalization step. Unlike previous PFL algorithms, PAC-PFL does not regularize all personalized models towards a single shared model, thereby greatly enhancing its personalization flexibility. By establishing and minimizing a PAC-Bayesian generalization bound on the average true loss of clients, PAC-PFL effectively mitigates overfitting even in data-poor scenarios. Additionally, PAC-PFL provides generalization bounds for new clients joining later. PAC-PFL achieves accurate and well-calibrated predictions, as supported by our experiments.",
        "published": "Thu, 27 Mar 2025 00:00:00 -0400",
        "source": "Security arxiv"
    }
}