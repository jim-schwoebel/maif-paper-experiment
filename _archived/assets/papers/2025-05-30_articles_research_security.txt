{
    "TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE": {
        "title": "TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE",
        "link": "https://arxiv.org/abs/2505.22735",
        "summary": "arXiv:2505.22735v1 Announce Type: new \nAbstract: To safeguard user data privacy, on-device inference has emerged as a prominent paradigm on mobile and Internet of Things (IoT) devices. This paradigm involves deploying a model provided by a third party on local devices to perform inference tasks. However, it exposes the private model to two primary security threats: model stealing (MS) and membership inference attacks (MIA). To mitigate these risks, existing wisdom deploys models within Trusted Execution Environments (TEEs), which is a secure isolated execution space. Nonetheless, the constrained secure memory capacity in TEEs makes it challenging to achieve full model security with low inference latency. This paper fills the gap with TensorShield, the first efficient on-device inference work that shields partial tensors of the model while still fully defending against MS and MIA. The key enabling techniques in TensorShield include: (i) a novel eXplainable AI (XAI) technique exploits the model's attention transition to assess critical tensors and shields them in TEE to achieve secure inference, and (ii) two meticulous designs with critical feature identification and latency-aware placement to accelerate inference while maintaining security. Extensive evaluations show that TensorShield delivers almost the same security protection as shielding the entire model inside TEE, while being up to 25.35$\\times$ (avg. 5.85$\\times$) faster than the state-of-the-art work, without accuracy loss.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Aurora: Are Android Malware Classifiers Reliable under Distribution Shift?": {
        "title": "Aurora: Are Android Malware Classifiers Reliable under Distribution Shift?",
        "link": "https://arxiv.org/abs/2505.22843",
        "summary": "arXiv:2505.22843v1 Announce Type: new \nAbstract: The performance figures of modern drift-adaptive malware classifiers appear promising, but does this translate to genuine operational reliability? The standard evaluation paradigm primarily focuses on baseline performance metrics, neglecting confidence-error alignment and operational stability. While TESSERACT established the importance of temporal evaluation, we take a complementary direction by investigating whether malware classifiers maintain reliable confidence estimates under distribution shifts and exploring the tensions between scientific advancement and practical impacts when they do not. We propose AURORA, a framework to evaluate malware classifiers based on their confidence quality and operational resilience. AURORA subjects the confidence profile of a given model to verification to assess the reliability of its estimates. Unreliable confidence estimates erode operational trust, waste valuable annotation budget on non-informative samples for active learning, and leave error-prone instances undetected in selective classification. AURORA is further complemented by a set of metrics designed to go beyond point-in-time performance, striving towards a more holistic assessment of operational stability throughout temporal evaluation periods. The fragility we observe in state-of-the-art frameworks across datasets of varying drift severity suggests the need for a return to the whiteboard.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Security Benefits and Side Effects of Labeling AI-Generated Images": {
        "title": "Security Benefits and Side Effects of Labeling AI-Generated Images",
        "link": "https://arxiv.org/abs/2505.22845",
        "summary": "arXiv:2505.22845v1 Announce Type: new \nAbstract: Generative artificial intelligence is developing rapidly, impacting humans' interaction with information and digital media. It is increasingly used to create deceptively realistic misinformation, so lawmakers have imposed regulations requiring the disclosure of AI-generated content. However, only little is known about whether these labels reduce the risks of AI-generated misinformation.\n  Our work addresses this research gap. Focusing on AI-generated images, we study the implications of labels, including the possibility of mislabeling. Assuming that simplicity, transparency, and trust are likely to impact the successful adoption of such labels, we first qualitatively explore users' opinions and expectations of AI labeling using five focus groups. Second, we conduct a pre-registered online survey with over 1300 U.S. and EU participants to quantitatively assess the effect of AI labels on users' ability to recognize misinformation containing either human-made or AI-generated images. Our focus groups illustrate that, while participants have concerns about the practical implementation of labeling, they consider it helpful in identifying AI-generated images and avoiding deception. However, considering security benefits, our survey revealed an ambiguous picture, suggesting that users might over-rely on labels. While inaccurate claims supported by labeled AI-generated images were rated less credible than those with unlabeled AI-images, the belief in accurate claims also decreased when accompanied by a labeled AI-generated image. Moreover, we find the undesired side effect that human-made images conveying inaccurate claims were perceived as more credible in the presence of labels.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment": {
        "title": "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment",
        "link": "https://arxiv.org/abs/2505.22852",
        "summary": "arXiv:2505.22852v1 Announce Type: new \nAbstract: CaMeL (Capabilities for Machine Learning) introduces a capability-based sandbox to mitigate prompt injection attacks in large language model (LLM) agents. While effective, CaMeL assumes a trusted user prompt, omits side-channel concerns, and incurs performance tradeoffs due to its dual-LLM design. This response identifies these issues and proposes engineering improvements to expand CaMeL's threat coverage and operational usability. We introduce: (1) prompt screening for initial inputs, (2) output auditing to detect instruction leakage, (3) a tiered-risk access model to balance usability and control, and (4) a verified intermediate language for formal guarantees. Together, these upgrades align CaMeL with best practices in enterprise security and support scalable deployment.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Permissioned LLMs: Enforcing Access Control in Large Language Models": {
        "title": "Permissioned LLMs: Enforcing Access Control in Large Language Models",
        "link": "https://arxiv.org/abs/2505.22860",
        "summary": "arXiv:2505.22860v1 Announce Type: new \nAbstract: In enterprise settings, organizational data is segregated, siloed and carefully protected by elaborate access control frameworks. These access control structures can completely break down if an LLM fine-tuned on the siloed data serves requests, for downstream tasks, from individuals with disparate access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs that superimpose the organizational data access control structures on query responses they generate. We formalize abstractions underpinning the means to determine whether access control enforcement happens correctly over LLM query responses. Our formalism introduces the notion of a relevant response that can be used to prove whether a PermLLM mechanism has been implemented correctly. We also introduce a novel metric, called access advantage, to empirically evaluate the efficacy of a PermLLM mechanism. We introduce three novel PermLLM mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired access control. We furthermore present two instantiations of access advantage--(i) Domain Distinguishability Index (DDI) based on Membership Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility evaluation. We demonstrate the efficacy of our PermLLM mechanisms through extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP), in addition to evaluating the validity of DDI and UGI metrics themselves for quantifying access control in LLMs.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection": {
        "title": "BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection",
        "link": "https://arxiv.org/abs/2505.22878",
        "summary": "arXiv:2505.22878v1 Announce Type: new \nAbstract: The current landscape of system-on-chips (SoCs) security verification faces challenges due to manual, labor-intensive, and inflexible methodologies. These issues limit the scalability and effectiveness of security protocols, making bug detection at the Register-Transfer Level (RTL) difficult. This paper proposes a new framework named BugWhisperer that utilizes a specialized, fine-tuned Large Language Model (LLM) to address these challenges. By enhancing the LLM's hardware security knowledge and leveraging its capabilities for text inference and knowledge transfer, this approach automates and improves the adaptability and reusability of the verification process. We introduce an open-source, fine-tuned LLM specifically designed for detecting security vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM effectively enhances the efficiency and flexibility of the security verification process. Additionally, we introduce a comprehensive hardware vulnerability database that supports this work and will further assist the research community in enhancing the security verification process.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Chainless Apps: A Modular Framework for Building Apps with Web2 Capability and Web3 Trust": {
        "title": "Chainless Apps: A Modular Framework for Building Apps with Web2 Capability and Web3 Trust",
        "link": "https://arxiv.org/abs/2505.22989",
        "summary": "arXiv:2505.22989v1 Announce Type: new \nAbstract: Modern blockchain applications are often constrained by a trade-off between user experience and trust. Chainless Apps present a new paradigm of application architecture that separates execution, trust, bridging, and settlement into distinct compostable layers. This enables app-specific sequencing, verifiable off-chain computation, chain-agnostic asset and message routing via Agglayer, and finality on Ethereum - resulting in fast Web2-like UX with Web3-grade verifiability. Although consensus mechanisms have historically underpinned verifiable computation, the advent of zkVMs and decentralized validation services opens up new trust models for developers. Chainless Apps leverage this evolution to offer modular, scalable applications that maintain interoperability with the broader blockchain ecosystem while allowing domain-specific trade-offs.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models": {
        "title": "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models",
        "link": "https://arxiv.org/abs/2505.23020",
        "summary": "arXiv:2505.23020v1 Announce Type: new \nAbstract: The acquisition of agentic capabilities has transformed LLMs from \"knowledge providers\" to \"action executors\", a trend that while expanding LLMs' capability boundaries, significantly increases their susceptibility to malicious use. Previous work has shown that current LLM-based agents execute numerous malicious tasks even without being attacked, indicating a deficiency in agentic use safety alignment during the post-training phase. To address this gap, we propose AgentAlign, a novel framework that leverages abstract behavior chains as a medium for safety alignment data synthesis. By instantiating these behavior chains in simulated environments with diverse tool instances, our framework enables the generation of highly authentic and executable instructions while capturing complex multi-step dynamics. The framework further ensures model utility by proportionally synthesizing benign instructions through non-malicious interpretations of behavior chains, precisely calibrating the boundary between helpfulness and harmlessness. Evaluation results on AgentHarm demonstrate that fine-tuning three families of open-source models using our method substantially improves their safety (35.8% to 79.5% improvement) while minimally impacting or even positively enhancing their helpfulness, outperforming various prompting methods. The dataset and code have both been open-sourced.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion": {
        "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion",
        "link": "https://arxiv.org/abs/2505.23266",
        "summary": "arXiv:2505.23266v1 Announce Type: new \nAbstract: We present Adversarial Object Fusion (AdvOF), a novel attack framework targeting vision-and-language navigation (VLN) agents in service-oriented environments by generating adversarial 3D objects. While foundational models like Large Language Models (LLMs) and Vision Language Models (VLMs) have enhanced service-oriented navigation systems through improved perception and decision-making, their integration introduces vulnerabilities in mission-critical service workflows. Existing adversarial attacks fail to address service computing contexts, where reliability and quality-of-service (QoS) are paramount. We utilize AdvOF to investigate and explore the impact of adversarial environments on the VLM-based perception module of VLN agents. In particular, AdvOF first precisely aggregates and aligns the victim object positions in both 2D and 3D space, defining and rendering adversarial objects. Then, we collaboratively optimize the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions. Through assigning importance weights to varying views, the optimization is processed stably and multi-viewedly by iterative fusions from local updates and justifications. Our extensive evaluations demonstrate AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks. This work advances the understanding of service security in VLM-powered navigation systems, providing computational foundations for robust service composition in physical-world deployments.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Joint Data Hiding and Partial Encryption of Compressive Sensed Streams": {
        "title": "Joint Data Hiding and Partial Encryption of Compressive Sensed Streams",
        "link": "https://arxiv.org/abs/2505.23357",
        "summary": "arXiv:2505.23357v1 Announce Type: new \nAbstract: The paper proposes a method to secure the Compressive Sensing (CS) streams. It consists in protecting part of the measurements by a secret key and inserting the code into the rest. The secret key is generated via a cryptographically secure pseudo-random number generator (CSPRNG) and XORed with the measurements to be inserted. For insertion, we use a reversible data hiding (RDH) scheme, which is a prediction error expansion algorithm, modified to match the statistics of CS measurements. The reconstruction from the embedded stream conducts to visibly distorted images. The image distortion is controlled by the number of embedded levels. In our tests, the embedding on 10 levels results in $\\approx 18 dB $ distortion for images of 256x256 pixels reconstructed with the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). A particularity of the presented method is on-the-fly insertion that makes it appropriate for the sequential acquisition of measurements by a Single Pixel Camera. On-the-fly insertion avoids the buffering of CS measurements for a subsequent standard encryption and generation of a thumbnail image.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models": {
        "title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models",
        "link": "https://arxiv.org/abs/2505.23561",
        "summary": "arXiv:2505.23561v1 Announce Type: new \nAbstract: Model merging for Large Language Models (LLMs) directly fuses the parameters of different models finetuned on various tasks, creating a unified model for multi-domain tasks. However, due to potential vulnerabilities in models available on open-source platforms, model merging is susceptible to backdoor attacks. In this paper, we propose Merge Hijacking, the first backdoor attack targeting model merging in LLMs. The attacker constructs a malicious upload model and releases it. Once a victim user merges it with any other models, the resulting merged model inherits the backdoor while maintaining utility across tasks. Merge Hijacking defines two main objectives-effectiveness and utility-and achieves them through four steps. Extensive experiments demonstrate the effectiveness of our attack across different models, merging algorithms, and tasks. Additionally, we show that the attack remains effective even when merging real-world models. Moreover, our attack demonstrates robustness against two inference-time defenses (Paraphrasing and CLEANGEN) and one training-time defense (Fine-pruning).",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Securing AI Agents with Information-Flow Control": {
        "title": "Securing AI Agents with Information-Flow Control",
        "link": "https://arxiv.org/abs/2505.23643",
        "summary": "arXiv:2505.23643v1 Announce Type: new \nAbstract: As AI agents become increasingly autonomous and capable, ensuring their security against vulnerabilities such as prompt injection becomes critical. This paper explores the use of information-flow control (IFC) to provide security guarantees for AI agents. We present a formal model to reason about the security and expressiveness of agent planners. Using this model, we characterize the class of properties enforceable by dynamic taint-tracking and construct a taxonomy of tasks to evaluate security and utility trade-offs of planner designs. Informed by this exploration, we present Fides, a planner that tracks confidentiality and integrity labels, deterministically enforces security policies, and introduces novel primitives for selectively hiding information. Its evaluation in AgentDojo demonstrates that this approach broadens the range of tasks that can be securely accomplished. A tutorial to walk readers through the the concepts introduced in the paper can be found at https://github.com/microsoft/fides",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference": {
        "title": "Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference",
        "link": "https://arxiv.org/abs/2505.23655",
        "summary": "arXiv:2505.23655v1 Announce Type: new \nAbstract: This work introduces a novel framework for secure and privacy-preserving neural network inference based on keyed chaotic dynamical transformations. The proposed method applies a deterministic, cryptographically seeded chaotic system to tensors, producing non-invertible, user-specific transformations that enable authenticated inference, tensor-level watermarking, and data attribution. This framework offers a scalable and lightweight alternative to conventional cryptographic techniques, and establishes a new direction for tensor-level security in AI systems.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Bayesian Perspective on Memorization and Reconstruction": {
        "title": "Bayesian Perspective on Memorization and Reconstruction",
        "link": "https://arxiv.org/abs/2505.23658",
        "summary": "arXiv:2505.23658v1 Announce Type: new \nAbstract: We introduce a new Bayesian perspective on the concept of data reconstruction, and leverage this viewpoint to propose a new security definition that, in certain settings, provably prevents reconstruction attacks. We use our paradigm to shed new light on one of the most notorious attacks in the privacy and memorization literature - fingerprinting code attacks (FPC). We argue that these attacks are really a form of membership inference attacks, rather than reconstruction attacks. Furthermore, we show that if the goal is solely to prevent reconstruction (but not membership inference), then in some cases the impossibility results derived from FPC no longer apply.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Private Rate-Constrained Optimization with Applications to Fair Learning": {
        "title": "Private Rate-Constrained Optimization with Applications to Fair Learning",
        "link": "https://arxiv.org/abs/2505.22703",
        "summary": "arXiv:2505.22703v1 Announce Type: cross \nAbstract: Many problems in trustworthy ML can be formulated as minimization of the model error under constraints on the prediction rates of the model for suitably-chosen marginals, including most group fairness constraints (demographic parity, equality of odds, etc.). In this work, we study such constrained minimization problems under differential privacy (DP). Standard DP optimization techniques like DP-SGD rely on the loss function's decomposability into per-sample contributions. However, rate constraints introduce inter-sample dependencies, violating the decomposability requirement. To address this, we develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA) algorithm which solves the Lagrangian formulation of rate constraint problems. We demonstrate that the additional privacy cost of incorporating these constraints reduces to privately estimating a histogram over the mini-batch at each optimization step. We prove the convergence of our algorithm through a novel analysis of SGDA that leverages the linear structure of the dual parameter. Finally, empirical results on learning under group fairness constraints demonstrate that our method Pareto-dominates existing private learning approaches in fairness-utility trade-offs.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Machine Learning Models Have a Supply Chain Problem": {
        "title": "Machine Learning Models Have a Supply Chain Problem",
        "link": "https://arxiv.org/abs/2505.22778",
        "summary": "arXiv:2505.22778v1 Announce Type: cross \nAbstract: Powerful machine learning (ML) models are now readily available online, which creates exciting possibilities for users who lack the deep technical expertise or substantial computing resources needed to develop them. On the other hand, this type of open ecosystem comes with many risks. In this paper, we argue that the current ecosystem for open ML models contains significant supply-chain risks, some of which have been exploited already in real attacks. These include an attacker replacing a model with something malicious (e.g., malware), or a model being trained using a vulnerable version of a framework or on restricted or poisoned data. We then explore how Sigstore, a solution designed to bring transparency to open-source software supply chains, can be used to bring transparency to open ML models, in terms of enabling model publishers to sign their models and prove properties about the datasets they use.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Efficient Preimage Approximation for Neural Network Certification": {
        "title": "Efficient Preimage Approximation for Neural Network Certification",
        "link": "https://arxiv.org/abs/2505.22798",
        "summary": "arXiv:2505.22798v1 Announce Type: cross \nAbstract: The growing reliance on artificial intelligence in safety- and security-critical applications demands effective neural network certification. A challenging real-world use case is certification against ``patch attacks'', where adversarial patches or lighting conditions obscure parts of images, for example traffic signs. One approach to certification, which also gives quantitative coverage estimates, utilizes preimages of neural networks, i.e., the set of inputs that lead to a specified output. However, these preimage approximation methods, including the state-of-the-art PREMAP algorithm, struggle with scalability. This paper presents novel algorithmic improvements to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved branching heuristics. We demonstrate efficiency improvements of at least an order of magnitude on reinforcement learning control benchmarks, and show that our method scales to convolutional neural networks that were previously infeasible. Our results demonstrate the potential of preimage approximation methodology for reliability and robustness certification.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks": {
        "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks",
        "link": "https://arxiv.org/abs/2505.23192",
        "summary": "arXiv:2505.23192v1 Announce Type: cross \nAbstract: The rise of text-to-image (T2I) models has enabled the synthesis of photorealistic human portraits, raising serious concerns about identity misuse and the robustness of AIGC detectors. In this work, we propose an automated adversarial prompt generation framework that leverages a grammar tree structure and a variant of the Monte Carlo tree search algorithm to systematically explore the semantic prompt space. Our method generates diverse, controllable prompts that consistently evade both open-source and commercial AIGC detectors. Extensive experiments across multiple T2I models validate its effectiveness, and the approach ranked first in a real-world adversarial AIGC detection competition. Beyond attack scenarios, our method can also be used to construct high-quality adversarial datasets, providing valuable resources for training and evaluating more robust AIGC detection and defense systems.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy": {
        "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
        "link": "https://arxiv.org/abs/2505.23397",
        "summary": "arXiv:2505.23397v1 Announce Type: cross \nAbstract: This article presents a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), integrating AI autonomy, trust calibration, and Human-in-the-loop decision making. Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI. Many assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration. To address these limitations, we propose a novel autonomy tiered framework grounded in five levels of AI autonomy from manual to fully autonomous, mapped to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This enables adaptive and explainable AI integration across core SOC functions, including monitoring, protection, threat detection, alert triage, and incident response. The proposed framework differentiates itself from previous research by creating formal connections between autonomy, trust, and HITL across various SOC levels, which allows for adaptive task distribution according to operational complexity and associated risks. The framework is exemplified through a simulated cyber range that features the cybersecurity AI-Avatar, a fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing response coordination, and strategically calibrating trust. This research systematically presents both the theoretical and practical aspects and feasibility of designing next-generation cognitive SOCs that leverage AI not to replace but to enhance human decision-making.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Quantum Hilbert Transform": {
        "title": "Quantum Hilbert Transform",
        "link": "https://arxiv.org/abs/2505.23581",
        "summary": "arXiv:2505.23581v1 Announce Type: cross \nAbstract: The Hilbert transform has been one of the foundational transforms in signal processing, finding it's way into multiple disciplines from cryptography to biomedical sciences. However, there does not exist any quantum analogue for the Hilbert transform. In this work, we introduce a formulation for the quantum Hilbert transform (QHT)and apply it to a quantum steganography protocol. By bridging classical phase-shift techniques with quantum operations, QHT opens new pathways in quantum signal processing, communications, sensing, and secure information hiding.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Towards A Global Quantum Internet: A Review of Challenges Facing Aerial Quantum Networks": {
        "title": "Towards A Global Quantum Internet: A Review of Challenges Facing Aerial Quantum Networks",
        "link": "https://arxiv.org/abs/2505.23603",
        "summary": "arXiv:2505.23603v1 Announce Type: cross \nAbstract: Quantum networks use principles of quantum physics to create secure communication networks. Moving these networks off the ground using drones, balloons, or satellites could help increase the scalability of these networks. This article reviews how such aerial links work, what makes them difficult to build, and the possible solutions that can be used to overcome these problems. By combining ground stations, aerial relays, and orbiting satellites into one seamless system, we move closer to a practical quantum internet.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment": {
        "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment",
        "link": "https://arxiv.org/abs/2505.23634",
        "summary": "arXiv:2505.23634v1 Announce Type: cross \nAbstract: The model context protocol (MCP) has been widely adapted as an open standard enabling the seamless integration of generative AI agents. However, recent work has shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks (FBAs), allowing malicious system access and credential theft, but requiring that users download compromised files directly to their systems. Herein, we show that the threat model of MCP-based attacks is significantly broader than previously thought, i.e., attackers need only post malicious content online to deceive MCP agents into carrying out their attacks on unsuspecting victims' systems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP dataset of FBAs and (truly) benign samples to explore the effectiveness of direct preference optimization (DPO) for the refusal training of large language models (LLMs). While DPO improves model guardrails against such attacks, we show that the efficacy of refusal learning varies drastically depending on the model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to refuse extremely poorly. Thus, to further improve FBA refusals, we introduce Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel preference alignment strategy based on RAG. We show that RAG-Pref significantly improves the ability of LLMs to refuse FBAs, particularly when combined with DPO alignment, thus drastically improving guardrails against MCP-based attacks.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Differentially Private Space-Efficient Algorithms for Counting Distinct Elements in the Turnstile Model": {
        "title": "Differentially Private Space-Efficient Algorithms for Counting Distinct Elements in the Turnstile Model",
        "link": "https://arxiv.org/abs/2505.23682",
        "summary": "arXiv:2505.23682v1 Announce Type: cross \nAbstract: The turnstile continual release model of differential privacy captures scenarios where a privacy-preserving real-time analysis is sought for a dataset evolving through additions and deletions. In typical applications of real-time data analysis, both the length of the stream $T$ and the size of the universe $|U|$ from which data come can be extremely large. This motivates the study of private algorithms in the turnstile setting using space sublinear in both $T$ and $|U|$. In this paper, we give the first sublinear space differentially private algorithms for the fundamental problem of counting distinct elements in the turnstile streaming model. Our algorithm achieves, on arbitrary streams, $\\tilde{O}_{\\eta}(T^{1/3})$ space and additive error, and a $(1+\\eta)$-relative approximation for all $\\eta \\in (0,1)$. Our result significantly improves upon the space requirements of the state-of-the-art algorithms for this problem, which is linear, approaching the known $\\Omega(T^{1/4})$ additive error lower bound for arbitrary streams. Moreover, when a bound $W$ on the number of times an item appears in the stream is known, our algorithm provides $\\tilde{O}_{\\eta}(\\sqrt{W})$ additive error, using $\\tilde{O}_{\\eta}(\\sqrt{W})$ space. This additive error asymptotically matches that of prior work which required instead linear space. Our results address an open question posed by [Jain, Kalemaj, Raskhodnikova, Sivakumar, Smith, Neurips23] about designing low-memory mechanisms for this problem. We complement these results with a space lower bound for this problem, which shows that any algorithm that uses similar techniques must use space $\\tilde{\\Omega}(T^{1/3})$ on arbitrary streams.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Concurrent Composition for Interactive Differential Privacy with Adaptive Privacy-Loss Parameters": {
        "title": "Concurrent Composition for Interactive Differential Privacy with Adaptive Privacy-Loss Parameters",
        "link": "https://arxiv.org/abs/2309.05901",
        "summary": "arXiv:2309.05901v2 Announce Type: replace \nAbstract: In this paper, we study the concurrent composition of interactive mechanisms with adaptively chosen privacy-loss parameters. In this setting, the adversary can interleave queries to existing interactive mechanisms, as well as create new ones. We prove that every valid privacy filter and odometer for noninteractive mechanisms extends to the concurrent composition of interactive mechanisms if privacy loss is measured using $(\\epsilon, \\delta)$-DP, $f$-DP, or R\\'enyi DP of fixed order. Our results offer strong theoretical foundations for enabling full adaptivity in composing differentially private interactive mechanisms, showing that concurrency does not affect the privacy guarantees. We also provide an implementation for users to deploy in practice.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Modelling 1/f Noise in TRNGs via Fractional Brownian Motion": {
        "title": "Modelling 1/f Noise in TRNGs via Fractional Brownian Motion",
        "link": "https://arxiv.org/abs/2410.14205",
        "summary": "arXiv:2410.14205v3 Announce Type: replace \nAbstract: Security of oscillatory true random number generators remains not fully understood due to insufficient understanding of complex $1/f^\\alpha$ phase noise. To bridge this gap, we introduce fractional Brownian motion as a comprehensive theoretical framework, capturing power-law spectral densities from white to flicker frequency noise.\n  Our key contributions provide closed-form tractable solutions: (1) a quasi-renewal property showing conditional variance grows with power-law time dependence, enabling tractable leakage analysis; (2) closed-form min-entropy expressions under Gaussian phase posteriors; and (3) asymptotically unbiased Allan variance parameter estimation.\n  This framework bridges physical modelling with cryptographic requirements, providing both theoretical foundations and practical calibration for oscillator-based TRNGs.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains": {
        "title": "SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains",
        "link": "https://arxiv.org/abs/2411.06426",
        "summary": "arXiv:2411.06426v3 Announce Type: replace \nAbstract: As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing": {
        "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
        "link": "https://arxiv.org/abs/2502.11647",
        "summary": "arXiv:2502.11647v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (Dynamic Editing for LLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Artemis: Toward Accurate Detection of Server-Side Request Forgeries through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis": {
        "title": "Artemis: Toward Accurate Detection of Server-Side Request Forgeries through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis",
        "link": "https://arxiv.org/abs/2502.21026",
        "summary": "arXiv:2502.21026v3 Announce Type: replace \nAbstract: Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web applications. Existing static tools in detecting vulnerabilities in PHP web applications neither contain SSRF-related features to enhance detection accuracy nor consider PHP's dynamic type features. In this paper, we present Artemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP web applications. First, Artemis extracts both PHP built-in and third-party functions as candidate source and sink functions. Second, Artemis constructs both explicit and implicit call graphs to infer functions' relationships. Third, Artemis performs taint analysis based on a set of rules that prevent over-tainting and pauses when SSRF exploitation is impossible. Fourth, Artemis analyzes the compatibility of path conditions to prune false positives. We have implemented a prototype of Artemis and evaluated it on 250 PHP web applications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with 15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported to developers, with 24 confirmed and assigned CVE IDs.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "MiZero: The Shadowy Defender Against Text Style Infringements": {
        "title": "MiZero: The Shadowy Defender Against Text Style Infringements",
        "link": "https://arxiv.org/abs/2504.00035",
        "summary": "arXiv:2504.00035v2 Announce Type: replace \nAbstract: In-Context Learning (ICL) and efficient fine-tuning methods significantly enhanced the efficiency of applying Large Language Models (LLMs) to downstream tasks. However, they also raise concerns about the imitation and infringement of personal creative data. Current methods for data copyright protection primarily focuses on content security but lacks effectiveness in protecting the copyrights of text styles. In this paper, we introduce a novel implicit zero-watermarking scheme, namely MiZero. This scheme establishes a precise watermark domain to protect the copyrighted style, surpassing traditional watermarking methods that distort the style characteristics. Specifically, we employ LLMs to extract condensed-lists utilizing the designed instance delimitation mechanism. These lists guide MiZero in generating the watermark. Extensive experiments demonstrate that MiZero effectively verifies text style copyright ownership against AI imitation.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control": {
        "title": "A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control",
        "link": "https://arxiv.org/abs/2505.19301",
        "summary": "arXiv:2505.19301v2 Announce Type: replace \nAbstract: Traditional Identity and Access Management (IAM) systems, primarily designed for human users or static machine identities via protocols such as OAuth, OpenID Connect (OIDC), and SAML, prove fundamentally inadequate for the dynamic, interdependent, and often ephemeral nature of AI agents operating at scale within Multi Agent Systems (MAS), a computational system composed of multiple interacting intelligent agents that work collectively.\n  This paper posits the imperative for a novel Agentic AI IAM framework: We deconstruct the limitations of existing protocols when applied to MAS, illustrating with concrete examples why their coarse-grained controls, single-entity focus, and lack of context-awareness falter. We then propose a comprehensive framework built upon rich, verifiable Agent Identities (IDs), leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), that encapsulate an agents capabilities, provenance, behavioral scope, and security posture.\n  Our framework includes an Agent Naming Service (ANS) for secure and capability-aware discovery, dynamic fine-grained access control mechanisms, and critically, a unified global session management and policy enforcement layer for real-time control and consistent revocation across heterogeneous agent communication protocols. We also explore how Zero-Knowledge Proofs (ZKPs) enable privacy-preserving attribute disclosure and verifiable policy compliance.\n  We outline the architecture, operational lifecycle, innovative contributions, and security considerations of this new IAM paradigm, aiming to establish the foundational trust, accountability, and security necessary for the burgeoning field of agentic AI and the complex ecosystems they will inhabit.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Hijacking Large Language Models via Adversarial In-Context Learning": {
        "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
        "link": "https://arxiv.org/abs/2311.09948",
        "summary": "arXiv:2311.09948v3 Announce Type: replace-cross \nAbstract: In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the preconditioned prompts. Despite its promising performance, crafted adversarial attacks pose a notable threat to the robustness of LLMs. Existing attacks are either easy to detect, require a trigger in user input, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable prompt injection attack against ICL, aiming to hijack LLMs to generate the target output or elicit harmful responses. In our threat model, the hacker acts as a model publisher who leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos via prompt injection. We also propose effective defense strategies using a few shots of clean demos, enhancing the robustness of LLMs during ICL. Extensive experimental results across various classification and jailbreak tasks demonstrate the effectiveness of the proposed attack and defense strategies. This work highlights the significant security vulnerabilities of LLMs during ICL and underscores the need for further in-depth studies.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Learning to Poison Large Language Models for Downstream Manipulation": {
        "title": "Learning to Poison Large Language Models for Downstream Manipulation",
        "link": "https://arxiv.org/abs/2402.13459",
        "summary": "arXiv:2402.13459v3 Announce Type: replace-cross \nAbstract: The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where the adversary inserts backdoor triggers into training data to manipulate outputs. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the supervised fine-tuning (SFT) process. We propose a novel gradient-guided backdoor trigger learning (GBTL) algorithm to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various language model tasks, including sentiment analysis, domain generation, and question answering, our poisoning strategy demonstrates a high success rate in compromising various LLMs' outputs. We further propose two defense strategies against data poisoning attacks, including in-context learning (ICL) and continuous learning (CL), which effectively rectify the behavior of LLMs and significantly reduce the decline in performance. Our work highlights the significant security risks present during SFT of LLMs and the necessity of safeguarding LLMs against data poisoning attacks.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates": {
        "title": "Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates",
        "link": "https://arxiv.org/abs/2402.17390",
        "summary": "arXiv:2402.17390v2 Announce Type: replace-cross \nAbstract: Machine-learning models demand periodic updates to improve their average accuracy, exploiting novel architectures and additional data. However, a newly updated model may commit mistakes the previous model did not make. Such misclassifications are referred to as negative flips, experienced by users as a regression of performance. In this work, we show that this problem also affects robustness to adversarial examples, hindering the development of secure model update practices. In particular, when updating a model to improve its adversarial robustness, previously ineffective adversarial attacks on some inputs may become successful, causing a regression in the perceived security of the system. We propose a novel technique, named robustness-congruent adversarial training, to address this issue. It amounts to fine-tuning a model with adversarial training, while constraining it to retain higher robustness on the samples for which no adversarial example was found before the update. We show that our algorithm and, more generally, learning with non-regression constraints, provides a theoretically-grounded framework to train consistent estimators. Our experiments on robust models for computer vision confirm that both accuracy and robustness, even if improved after model update, can be affected by negative flips, and our robustness-congruent adversarial training can mitigate the problem, outperforming competing baseline methods.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model": {
        "title": "Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model",
        "link": "https://arxiv.org/abs/2405.14457",
        "summary": "arXiv:2405.14457v3 Announce Type: replace-cross \nAbstract: Machine learning models can be trained with formal privacy guarantees via differentially private optimizers such as DP-SGD. In this work, we focus on a threat model where the adversary has access only to the final model, with no visibility into intermediate updates. In the literature, this hidden state threat model exhibits a significant gap between the lower bound from empirical privacy auditing and the theoretical upper bound provided by privacy accounting. To challenge this gap, we propose to audit this threat model with adversaries that craft a gradient sequence designed to maximize the privacy loss of the final model without relying on intermediate updates. Our experiments show that this approach consistently outperforms previous attempts at auditing the hidden state model. Furthermore, our results advance the understanding of achievable privacy guarantees within this threat model. Specifically, when the crafted gradient is inserted at every optimization step, we show that concealing the intermediate model updates in DP-SGD does not enhance the privacy guarantees. The situation is more complex when the crafted gradient is not inserted at every step: our auditing lower bound matches the privacy upper bound only for an adversarially-chosen loss landscape and a sufficiently large batch size. This suggests that existing privacy upper bounds can be improved in certain regimes.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Are You Using Reliable Graph Prompts? Trojan Prompt Attacks on Graph Neural Networks": {
        "title": "Are You Using Reliable Graph Prompts? Trojan Prompt Attacks on Graph Neural Networks",
        "link": "https://arxiv.org/abs/2410.13974",
        "summary": "arXiv:2410.13974v2 Announce Type: replace-cross \nAbstract: Graph Prompt Learning (GPL) has been introduced as a promising approach that uses prompts to adapt pre-trained GNN models to specific downstream tasks without requiring fine-tuning of the entire model. Despite the advantages of GPL, little attention has been given to its vulnerability to backdoor attacks, where an adversary can manipulate the model's behavior by embedding hidden triggers. Existing graph backdoor attacks rely on modifying model parameters during training, but this approach is impractical in GPL as GNN encoder parameters are frozen after pre-training. Moreover, downstream users may fine-tune their own task models on clean datasets, further complicating the attack. In this paper, we propose TGPA, a backdoor attack framework designed specifically for GPL. TGPA injects backdoors into graph prompts without modifying pre-trained GNN encoders and ensures high attack success rates and clean accuracy. To address the challenge of model fine-tuning by users, we introduce a finetuning-resistant poisoning approach that maintains the effectiveness of the backdoor even after downstream model adjustments. Extensive experiments on multiple datasets under various settings demonstrate the effectiveness of TGPA in compromising GPL models with fixed GNN encoders.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks": {
        "title": "One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks",
        "link": "https://arxiv.org/abs/2410.22725",
        "summary": "arXiv:2410.22725v4 Announce Type: replace-cross \nAbstract: Recently, various types of Text-to-Image (T2I) models have emerged (such as DALL-E and Stable Diffusion), and showing their advantages in different aspects. Therefore, some third-party service platforms collect different model interfaces and provide cheaper API services and more flexibility in T2I model selections. However, this also raises a new security concern: Are these third-party services truly offering the models they claim?\n  To answer this question, we first define the concept of T2I model verification, which aims to determine whether a black-box target model is identical to a given white-box reference T2I model. After that, we propose VerifyPrompt, which performs T2I model verification through a special designed verify prompt. Intuitionally, the verify prompt is an adversarial prompt for the target model without transferability for other models. It makes the target model generate a specific image while making other models produce entirely different images. Specifically, VerifyPrompt utilizes the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize the cosine similarity of a prompt's text encoding, generating verify prompts. Finally, by computing the CLIP-text similarity scores between the prompts the generated images, VerifyPrompt can determine whether the target model aligns with the reference model. Experimental results demonstrate that VerifyPrompt consistently achieves over 90\\% accuracy across various T2I models, confirming its effectiveness in practical model platforms (such as Hugging Face).",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting": {
        "title": "Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting",
        "link": "https://arxiv.org/abs/2502.02410",
        "summary": "arXiv:2502.02410v2 Announce Type: replace-cross \nAbstract: Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with time-series-specific tasks like forecasting, since they rely on the privacy amplification attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this structured subsampling to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Privacy-Aware Joint DNN Model Deployment and Partitioning Optimization for Collaborative Edge Inference Services": {
        "title": "Privacy-Aware Joint DNN Model Deployment and Partitioning Optimization for Collaborative Edge Inference Services",
        "link": "https://arxiv.org/abs/2502.16091",
        "summary": "arXiv:2502.16091v3 Announce Type: replace-cross \nAbstract: Edge inference (EI) has emerged as a promising paradigm to address the growing limitations of cloud-based Deep Neural Network (DNN) inference services, such as high response latency, limited scalability, and severe data privacy exposure. However, deploying DNN models on resource-constrained edge devices introduces additional challenges, including limited computation/storage resources, dynamic service demands, and heightened privacy risks. To tackle these issues, this paper presents a novel privacy-aware optimization framework that jointly addresses DNN model deployment, user-server association, and model partitioning, with the goal of minimizing long-term average inference delay under resource and privacy constraints. The problem is formulated as a complex, NP-hard stochastic optimization. To efficiently handle system dynamics and computational complexity, we employ a Lyapunov-based approach to transform the long-term objective into tractable per-slot decisions. Furthermore, we introduce a coalition formation game to enable adaptive user-server association and design a greedy algorithm for model deployment within each coalition. Extensive simulations demonstrate that the proposed algorithm significantly reduces inference delay and consistently satisfies privacy constraints, outperforming state-of-the-art baselines across diverse scenarios.",
        "published": "Fri, 30 May 2025 00:00:00 -0400",
        "source": "Security arxiv"
    }
}