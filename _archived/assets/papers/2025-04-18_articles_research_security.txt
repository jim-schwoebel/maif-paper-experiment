{
    "Decentralised collaborative action: cryptoeconomics in space": {
        "title": "Decentralised collaborative action: cryptoeconomics in space",
        "link": "https://arxiv.org/abs/2504.12493",
        "summary": "arXiv:2504.12493v1 Announce Type: new \nAbstract: Blockchains and peer-to-peer systems are part of a trend towards computer systems that are \"radically decentralised\", by which we mean that they 1) run across many participants, 2) without central control, and 3) are such that qualities 1 and 2 are essential to the system's intended use cases.\n  We propose a notion of topological space, which we call a \"semitopology\", to help us mathematically model such systems. We treat participants as points in a space, which are organised into \"actionable coalitions\". An actionable coalition is any set of participants who collectively have the resources to collaborate (if they choose) to progress according to the system's rules, without involving any other participants in the system.\n  It turns out that much useful information about the system can be obtained \\emph{just} by viewing it as a semitopology and studying its actionable coalitions. For example: we will prove a mathematical sense in which if every actionable coalition of some point p has nonempty intersection with every actionable coalition of another point q -- note that this is the negation of the famous Hausdorff separation property from topology -- then p and q must remain in agreement.\n  This is of practical interest, because remaining in agreement is a key correctness property in many distributed systems. For example in blockchain, participants disagreeing is called \"forking\", and blockchain designers try hard to avoid it.\n  We provide an accessible introduction to: the technical context of decentralised systems; why we build them and find them useful; how they motivate the theory of semitopological spaces; and we sketch some basic theorems and applications of the resulting mathematics.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Provable Secure Steganography Based on Adaptive Dynamic Sampling": {
        "title": "Provable Secure Steganography Based on Adaptive Dynamic Sampling",
        "link": "https://arxiv.org/abs/2504.12579",
        "summary": "arXiv:2504.12579v1 Announce Type: new \nAbstract: The security of private communication is increasingly at risk due to widespread surveillance. Steganography, a technique for embedding secret messages within innocuous carriers, enables covert communication over monitored channels. Provably Secure Steganography (PSS) is state of the art for making stego carriers indistinguishable from normal ones by ensuring computational indistinguishability between stego and cover distributions. However, current PSS methods often require explicit access to the distribution of generative model for both sender and receiver, limiting their practicality in black box scenarios. In this paper, we propose a provably secure steganography scheme that does not require access to explicit model distributions for both sender and receiver. Our method incorporates a dynamic sampling strategy, enabling generative models to embed secret messages within multiple sampling choices without disrupting the normal generation process of the model. Extensive evaluations of three real world datasets and three LLMs demonstrate that our blackbox method is comparable with existing white-box steganography methods in terms of efficiency and capacity while eliminating the degradation of steganography in model generated outputs.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Privacy-Preserving CNN Training with Transfer Learning: Two Hidden Layers": {
        "title": "Privacy-Preserving CNN Training with Transfer Learning: Two Hidden Layers",
        "link": "https://arxiv.org/abs/2504.12623",
        "summary": "arXiv:2504.12623v1 Announce Type: new \nAbstract: In this paper, we present the demonstration of training a four-layer neural network entirely using fully homomorphic encryption (FHE), supporting both single-output and multi-output classification tasks in a non-interactive setting. A key contribution of our work is identifying that replacing \\textit{Softmax} with \\textit{Sigmoid}, in conjunction with the Binary Cross-Entropy (BCE) loss function, provides an effective and scalable solution for homomorphic classification. Moreover, we show that the BCE loss function, originally designed for multi-output tasks, naturally extends to the multi-class setting, thereby enabling broader applicability. We also highlight the limitations of prior loss functions such as the SLE loss and the one proposed in the 2019 CVPR Workshop, both of which suffer from vanishing gradients as network depth increases. To address the challenges posed by large-scale encrypted data, we further introduce an improved version of the previously proposed data encoding scheme, \\textit{Double Volley Revolver}, which achieves a better trade-off between computational and memory efficiency, making FHE-based neural network training more practical. The complete, runnable C++ code to implement our work can be found at: \\href{https://github.com/petitioner/ML.NNtraining}{$\\texttt{https://github.com/petitioner/ML.NNtraining}$}.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Malicious Code Detection in Smart Contracts via Opcode Vectorization": {
        "title": "Malicious Code Detection in Smart Contracts via Opcode Vectorization",
        "link": "https://arxiv.org/abs/2504.12720",
        "summary": "arXiv:2504.12720v1 Announce Type: new \nAbstract: With the booming development of blockchain technology, smart contracts have been widely used in finance, supply chain, Internet of things and other fields in recent years. However, the security problems of smart contracts become increasingly prominent. Security events caused by smart contracts occur frequently, and the existence of malicious codes may lead to the loss of user assets and system crash. In this paper, a simple study is carried out on malicious code detection of intelligent contracts based on machine learning. The main research work and achievements are as follows: Feature extraction and vectorization of smart contract are the first step to detect malicious code of smart contract by using machine learning method, and feature processing has an important impact on detection results. In this paper, an opcode vectorization method based on smart contract text is adopted. Based on considering the structural characteristics of contract opcodes, the opcodes are classified and simplified. Then, N-Gram (N=2) algorithm and TF-IDF algorithm are used to convert the simplified opcodes into vectors, and then put into the machine learning model for training. In contrast, N-Gram algorithm and TF-IDF algorithm are directly used to quantify opcodes and put into the machine learning model training. Judging which feature extraction method is better according to the training results. Finally, the classifier chain is applied to the intelligent contract malicious code detection.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric": {
        "title": "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric",
        "link": "https://arxiv.org/abs/2504.12733",
        "summary": "arXiv:2504.12733v1 Announce Type: new \nAbstract: This paper presents an adversary model and a simulation framework specifically tailored for analyzing attacks on distributed systems composed of multiple distributed protocols, with a focus on assessing the security of blockchain networks. Our model classifies and constrains adversarial actions based on the assumptions of the target protocols, defined by failure models, communication models, and the fault tolerance thresholds of Byzantine Fault Tolerant (BFT) protocols. The goal is to study not only the intended effects of adversarial strategies but also their unintended side effects on critical system properties. We apply this framework to analyze fairness properties in a Hyperledger Fabric (HF) blockchain network. Our focus is on novel fairness attacks that involve coordinated adversarial actions across various HF services. Simulations show that even a constrained adversary can violate fairness with respect to specific clients (client fairness) and impact related guarantees (order fairness), which relate the reception order of transactions to their final order in the blockchain. This paper significantly extends our previous work by introducing and evaluating a mitigation mechanism specifically designed to counter transaction reordering attacks. We implement and integrate this defense into our simulation environment, demonstrating its effectiveness under diverse conditions.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Attack-Defense Trees with Offensive and Defensive Attributes (with Appendix)": {
        "title": "Attack-Defense Trees with Offensive and Defensive Attributes (with Appendix)",
        "link": "https://arxiv.org/abs/2504.12748",
        "summary": "arXiv:2504.12748v1 Announce Type: new \nAbstract: Effective risk management in cybersecurity requires a thorough understanding of the interplay between attacker capabilities and defense strategies. Attack-Defense Trees (ADTs) are a commonly used methodology for representing this interplay; however, previous work in this domain has only focused on analyzing metrics such as cost, damage, or time from the perspective of the attacker. This approach provides an incomplete view of the system, as it neglects to model defender attributes: in real-world scenarios, defenders have finite resources for countermeasures and are similarly constrained. In this paper, we propose a novel framework that incorporates defense metrics into ADTs, and we present efficient algorithms for computing the Pareto front between defense and attack metrics. Our methods encode both attacker and defender metrics as semirings, allowing our methods to be used for many metrics such as cost, damage, and skill. We analyze tree-structured ADTs using a bottom-up approach and general ADTs by translating them into binary decision diagrams. Experiments on randomly generated ADTS demonstrate that both approaches effectively handle ADTs with several hundred nodes.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System": {
        "title": "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System",
        "link": "https://arxiv.org/abs/2504.12757",
        "summary": "arXiv:2504.12757v1 Announce Type: new \nAbstract: As Agentic AI gain mainstream adoption, the industry invests heavily in model capabilities, achieving rapid leaps in reasoning and quality. However, these systems remain largely confined to data silos, and each new integration requires custom logic that is difficult to scale. The Model Context Protocol (MCP) addresses this challenge by defining a universal, open standard for securely connecting AI-based applications (MCP clients) to data sources (MCP servers). However, the flexibility of the MCP introduces new risks, including malicious tool servers and compromised data integrity. We present MCP Guardian, a framework that strengthens MCP-based communication with authentication, rate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning. Through real-world scenarios and empirical testing, we demonstrate how MCP Guardian effectively mitigates attacks and ensures robust oversight with minimal overheads. Our approach fosters secure, scalable data access for AI assistants, underscoring the importance of a defense-in-depth approach that enables safer and more transparent innovation in AI-driven environments.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "SoK: Security of EMV Contactless Payment Systems": {
        "title": "SoK: Security of EMV Contactless Payment Systems",
        "link": "https://arxiv.org/abs/2504.12812",
        "summary": "arXiv:2504.12812v1 Announce Type: new \nAbstract: The widespread adoption of EMV (Europay, Mastercard, and Visa) contactless payment systems has greatly improved convenience for both users and merchants. However, this growth has also exposed significant security challenges. This SoK provides a comprehensive analysis of security vulnerabilities in EMV contactless payments, particularly within the open-loop systems used by Visa and Mastercard. We categorize attacks into seven attack vectors across three key areas: application selection, cardholder authentication, and transaction authorization. We replicate the attacks on Visa and Mastercard protocols using our experimental platform to determine their practical feasibility and offer insights into the current security landscape of contactless payments. Our study also includes a detailed evaluation of the underlying protocols, along with a comparative analysis of Visa and Mastercard, highlighting vulnerabilities and recommending countermeasures.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "GraphAttack: Exploiting Representational Blindspots in LLM Safety Mechanisms": {
        "title": "GraphAttack: Exploiting Representational Blindspots in LLM Safety Mechanisms",
        "link": "https://arxiv.org/abs/2504.13052",
        "summary": "arXiv:2504.13052v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have been equipped with safety mechanisms to prevent harmful outputs, but these guardrails can often be bypassed through \"jailbreak\" prompts. This paper introduces a novel graph-based approach to systematically generate jailbreak prompts through semantic transformations. We represent malicious prompts as nodes in a graph structure with edges denoting different transformations, leveraging Abstract Meaning Representation (AMR) and Resource Description Framework (RDF) to parse user goals into semantic components that can be manipulated to evade safety filters. We demonstrate a particularly effective exploitation vector by instructing LLMs to generate code that realizes the intent described in these semantic graphs, achieving success rates of up to 87% against leading commercial LLMs. Our analysis reveals that contextual framing and abstraction are particularly effective at circumventing safety measures, highlighting critical gaps in current safety alignment techniques that focus primarily on surface-level patterns. These findings provide insights for developing more robust safeguards against structured semantic attacks. Our research contributes both a theoretical framework and practical methodology for systematically stress-testing LLM safety mechanisms.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Anonymous Public Announcements": {
        "title": "Anonymous Public Announcements",
        "link": "https://arxiv.org/abs/2504.12546",
        "summary": "arXiv:2504.12546v1 Announce Type: cross \nAbstract: We formalise the notion of an \\emph{anonymous public announcement} in the tradition of public announcement logic. Such announcements can be seen as in-between a public announcement from ``the outside\" (an announcement of $\\phi$) and a public announcement by one of the agents (an announcement of $K_a\\phi$): we get more information than just $\\phi$, but not (necessarily) about exactly who made it. Even if such an announcement is prima facie anonymous, depending on the background knowledge of the agents it might reveal the identity of the announcer: if I post something on a message board, the information might reveal who I am even if I don't sign my name. Furthermore, like in the Russian Cards puzzle, if we assume that the announcer's intention was to stay anonymous, that in fact might reveal more information. In this paper we first look at the case when no assumption about intentions are made, in which case the logic with an anonymous public announcement operator is reducible to epistemic logic. We then look at the case when we assume common knowledge of the intention to stay anonymous, which is both more complex and more interesting: in several ways it boils down to the notion of a ``safe\" announcement (again, similarly to Russian Cards). Main results include formal expressivity results and axiomatic completeness for key logical languages.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients": {
        "title": "Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients",
        "link": "https://arxiv.org/abs/2504.12577",
        "summary": "arXiv:2504.12577v1 Announce Type: cross \nAbstract: Federated learning (FL) enables collaborative training of deep learning models without requiring data to leave local clients, thereby preserving client privacy. The aggregation process on the server plays a critical role in the performance of the resulting FL model. The most commonly used aggregation method is weighted averaging based on the amount of data from each client, which is thought to reflect each client's contribution. However, this method is prone to model bias, as dishonest clients might report inaccurate training data volumes to the server, which is hard to verify. To address this issue, we propose a novel secure \\underline{Fed}erated \\underline{D}ata q\\underline{u}antity-\\underline{a}ware weighted averaging method (FedDua). It enables FL servers to accurately predict the amount of training data from each client based on their local model gradients uploaded. Furthermore, it can be seamlessly integrated into any FL algorithms that involve server-side model aggregation. Extensive experiments on three benchmarking datasets demonstrate that FedDua improves the global model performance by an average of 3.17% compared to four popular FL aggregation methods in the presence of inaccurate client data volume declarations.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Codes over Finite Ring $\\mathbb{Z}_k$, MacWilliams Identity and Theta Function": {
        "title": "Codes over Finite Ring $\\mathbb{Z}_k$, MacWilliams Identity and Theta Function",
        "link": "https://arxiv.org/abs/2504.12604",
        "summary": "arXiv:2504.12604v1 Announce Type: cross \nAbstract: In this paper, we study linear codes over $\\mathbb{Z}_k$ based on lattices and theta functions. We obtain the complete weight enumerators MacWilliams identity and the symmetrized weight enumerators MacWilliams identity based on the theory of theta function. We extend the main work by Bannai, Dougherty, Harada and Oura to the finite ring $\\mathbb{Z}_k$ for any positive integer $k$ and present the complete weight enumerators MacWilliams identity in genus $g$. When $k=p$ is a prime number, we establish the relationship between the theta function of associated lattices over a cyclotomic field and the complete weight enumerators with Hamming weight of codes, which is an analogy of the results by G. Van der Geer and F. Hirzebruch since they showed the identity with the Lee weight enumerators.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance": {
        "title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance",
        "link": "https://arxiv.org/abs/2504.12612",
        "summary": "arXiv:2504.12612v1 Announce Type: cross \nAbstract: Provenance is the chronology of things, resonating with the fundamental pursuit to uncover origins, trace connections, and situate entities within the flow of space and time. As artificial intelligence advances towards autonomous agents capable of interactive collaboration on complex tasks, the provenance of generated content becomes entangled in the interplay of collective creation, where contributions are continuously revised, extended or overwritten. In a multi-agent generative chain, content undergoes successive transformations, often leaving little, if any, trace of prior contributions. In this study, we investigates the problem of tracking multi-agent provenance across the temporal dimension of generation. We propose a chronological system for post hoc attribution of generative history from content alone, without reliance on internal memory states or external meta-information. At its core lies the notion of symbolic chronicles, representing signed and time-stamped records, in a form analogous to the chain of custody in forensic science. The system operates through a feedback loop, whereby each generative timestep updates the chronicle of prior interactions and synchronises it with the synthetic content in the very act of generation. This research seeks to develop an accountable form of collaborative artificial intelligence within evolving cyber ecosystems.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification": {
        "title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification",
        "link": "https://arxiv.org/abs/2504.12644",
        "summary": "arXiv:2504.12644v1 Announce Type: cross \nAbstract: Deep learning (DL)-based image classification models are essential for autonomous vehicle (AV) perception modules since incorrect categorization might have severe repercussions. Adversarial attacks are widely studied cyberattacks that can lead DL models to predict inaccurate output, such as incorrectly classified traffic signs by the perception module of an autonomous vehicle. In this study, we create and compare hybrid classical-quantum deep learning (HCQ-DL) models with classical deep learning (C-DL) models to demonstrate robustness against adversarial attacks for perception modules. Before feeding them into the quantum system, we used transfer learning models, alexnet and vgg-16, as feature extractors. We tested over 1000 quantum circuits in our HCQ-DL models for projected gradient descent (PGD), fast gradient sign attack (FGSA), and gradient attack (GA), which are three well-known untargeted adversarial approaches. We evaluated the performance of all models during adversarial attacks and no-attack scenarios. Our HCQ-DL models maintain accuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA attacks, which is higher than C-DL models. During the PGD attack, our alexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL models that achieved accuracies below 21\\%. Our results highlight that the HCQ-DL models provide improved accuracy for traffic sign classification under adversarial settings compared to their classical counterparts.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts": {
        "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts",
        "link": "https://arxiv.org/abs/2504.12782",
        "summary": "arXiv:2504.12782v1 Announce Type: cross \nAbstract: Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer a promising solution, existing finetuning-based approaches suffer from notable limitations. Anchor-free methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce a finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on a key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires a trajectory-aware objective that preserves the integrity of the early-stage score function field, which steers samples toward the natural image manifold, without relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentation-enhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers a versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks": {
        "title": "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks",
        "link": "https://arxiv.org/abs/2504.12806",
        "summary": "arXiv:2504.12806v1 Announce Type: cross \nAbstract: The loss landscape of Variational Quantum Neural Networks (VQNNs) is characterized by local minima that grow exponentially with increasing qubits. Because of this, it is more challenging to recover information from model gradients during training compared to classical Neural Networks (NNs). In this paper we present a numerical scheme that successfully reconstructs input training, real-world, practical data from trainable VQNNs' gradients. Our scheme is based on gradient inversion that works by combining gradients estimation with the finite difference method and adaptive low-pass filtering. The scheme is further optimized with Kalman filter to obtain efficient convergence. Our experiments show that our algorithm can invert even batch-trained data, given the VQNN model is sufficiently over-parameterized.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Algorithms for the Shortest Vector Problem in $2$-dimensional Lattices, Revisited": {
        "title": "Algorithms for the Shortest Vector Problem in $2$-dimensional Lattices, Revisited",
        "link": "https://arxiv.org/abs/2504.12948",
        "summary": "arXiv:2504.12948v1 Announce Type: cross \nAbstract: Efficiently solving the Shortest Vector Problem (SVP) in two-dimensional lattices holds practical significance in cryptography and computational geometry. While simpler than its high-dimensional counterpart, two-dimensional SVP motivates scalable solutions for high-dimensional lattices and benefits applications like sequence cipher cryptanalysis involving large integers. In this work, we first propose a novel definition of reduced bases and develop an efficient adaptive lattice reduction algorithm \\textbf{CrossEuc} that strategically applies the Euclidean algorithm across dimensions. Building on this framework, we introduce \\textbf{HVec}, a vectorized generalization of the Half-GCD algorithm originally defined for integers, which can efficiently halve the bit-length of two vectors and may have independent interest. By iteratively invoking \\textbf{HVec}, our optimized algorithm \\textbf{HVecSBP} achieves a reduced basis in $O(\\log n M(n) )$ time for arbitrary input bases with bit-length $n$, where $M(n)$ denotes the cost of multiplying two $n$-bit integers. Compared to existing algorithms, our design is applicable to general forms of input lattices, eliminating the cost of pre-converting input bases to Hermite Normal Form (HNF). The comprehensive experimental results demonstrate that for the input lattice bases in HNF, the optimized algorithm \\textbf{HVecSBP} achieves at least a $13.5\\times$ efficiency improvement compared to existing methods. For general-form input lattice bases, converting them to HNF before applying \\textbf{HVecSBP} offers only marginal advantages in extreme cases where the two basis vectors are nearly degenerate. However, as the linear dependency between input basis vectors decreases, directly employing \\textbf{HVecSBP} yields increasingly significant efficiency gains, outperforming hybrid approaches that rely on prior \\textbf{HNF} conversion.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models": {
        "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
        "link": "https://arxiv.org/abs/2504.13061",
        "summary": "arXiv:2504.13061v1 Announce Type: cross \nAbstract: Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n  To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "An Analysis of Malicious Packages in Open-Source Software in the Wild": {
        "title": "An Analysis of Malicious Packages in Open-Source Software in the Wild",
        "link": "https://arxiv.org/abs/2404.04991",
        "summary": "arXiv:2404.04991v3 Announce Type: replace \nAbstract: The open-source software (OSS) ecosystem suffers from security threats caused by malware.However, OSS malware research has three limitations: a lack of high-quality datasets, a lack of malware diversity, and a lack of attack campaign contexts. In this paper, we first build the largest dataset of 24,356 malicious packages from online sources, then propose a knowledge graph to represent the OSS malware corpus and conduct malware analysis in the wild.Our main findings include (1) it is essential to collect malicious packages from various online sources because their data overlapping degrees are small;(2) despite the sheer volume of malicious packages, many reuse similar code, leading to a low diversity of malware;(3) only 28 malicious packages were repeatedly hidden via dependency libraries of 1,354 malicious packages, and dependency-hidden malware has a shorter active time;(4) security reports are the only reliable source for disclosing the malware-based context. Index Terms: Malicious Packages, Software Analysis",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "From Sands to Mansions: Towards Automated Cyberattack Emulation with Classical Planning and Large Language Models": {
        "title": "From Sands to Mansions: Towards Automated Cyberattack Emulation with Classical Planning and Large Language Models",
        "link": "https://arxiv.org/abs/2407.16928",
        "summary": "arXiv:2407.16928v3 Announce Type: replace \nAbstract: As attackers continually advance their tools, skills, and techniques during cyberattacks - particularly in modern Advanced Persistence Threats (APT) campaigns - there is a pressing need for a comprehensive and up-to-date cyberattack dataset to support threat-informed defense and enable benchmarking of defense systems in both academia and commercial solutions. However, there is a noticeable scarcity of cyberattack datasets: recent academic studies continue to rely on outdated benchmarks, while cyberattack emulation in industry remains limited due to the significant human effort and expertise required. Creating datasets by emulating advanced cyberattacks presents several challenges, such as limited coverage of attack techniques, the complexity of chaining multiple attack steps, and the difficulty of realistically mimicking actual threat groups. In this paper, we introduce modularized Attack Action and Attack Action Linking Model as a structured way to organizing and chaining individual attack steps into multi-step cyberattacks. Building on this, we propose Aurora, a system that autonomously emulates cyberattacks using third-party attack tools and threat intelligence reports with the help of classical planning and large language models. Aurora can automatically generate detailed attack plans, set up emulation environments, and semi-automatically execute the attacks. We utilize Aurora to create a dataset containing over 1,000 attack chains. To our best knowledge, Aurora is the only system capable of automatically constructing such a large-scale cyberattack dataset with corresponding attack execution scripts and environments. Our evaluation further demonstrates that Aurora outperforms the previous similar work and even the most advanced generative AI models in cyberattack emulation. To support further research, we published the cyberattack dataset and will publish the source code of Aurora.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "PhishLang: A Real-Time, Fully Client-Side Phishing Detection Framework Using MobileBERT": {
        "title": "PhishLang: A Real-Time, Fully Client-Side Phishing Detection Framework Using MobileBERT",
        "link": "https://arxiv.org/abs/2408.05667",
        "summary": "arXiv:2408.05667v3 Announce Type: replace \nAbstract: In this paper, we introduce PhishLang, the first fully client-side anti-phishing framework built on a lightweight ensemble framework that utilizes advanced language models to analyze the contextual features of a website's source code and URL. Unlike traditional heuristic or machine learning approaches that rely on static features and struggle to adapt to evolving threats, or deep learning models that are computationally intensive, our approach utilizes MobileBERT, a fast and memory-efficient variant of the BERT architecture, to capture nuanced features indicative of phishing attacks. To further enhance detection accuracy, PhishLang employs a multi-modal ensemble approach, combining both the URL and Source detection models. This architecture ensures robustness by allowing one model to compensate for scenarios where the other may fail, or if both models provide ambiguous inferences. As a result, PhishLang excels at detecting both regular and evasive phishing threats, including zero-day attacks, outperforming popular anti-phishing tools, while operating without relying on external blocklists and safeguarding user privacy by ensuring that browser history remains entirely local and unshared. We release PhishLang as a Chromium browser extension and also open-source the framework to aid the research community.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "MalMixer: Few-Shot Malware Classification with Retrieval-Augmented Semi-Supervised Learning": {
        "title": "MalMixer: Few-Shot Malware Classification with Retrieval-Augmented Semi-Supervised Learning",
        "link": "https://arxiv.org/abs/2409.13213",
        "summary": "arXiv:2409.13213v4 Announce Type: replace \nAbstract: Recent growth and proliferation of malware have tested practitioners ability to promptly classify new samples according to malware families. In contrast to labor-intensive reverse engineering efforts, machine learning approaches have demonstrated increased speed and accuracy. However, most existing deep-learning malware family classifiers must be calibrated using a large number of samples that are painstakingly manually analyzed before training. Furthermore, as novel malware samples arise that are beyond the scope of the training set, additional reverse engineering effort must be employed to update the training set. The sheer volume of new samples found in the wild creates substantial pressure on practitioners ability to reverse engineer enough malware to adequately train modern classifiers. In this paper, we present MalMixer, a malware family classifier using semi-supervised learning that achieves high accuracy with sparse training data. We present a domain-knowledge-aware data augmentation technique for malware feature representations, enhancing few-shot performance of semi-supervised malware family classification. We show that MalMixer achieves state-of-the-art performance in few-shot malware family classification settings. Our research confirms the feasibility and effectiveness of lightweight, domain-knowledge-aware data augmentation methods for malware features and shows the capabilities of similar semi-supervised classifiers in addressing malware classification issues.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Harmless Backdoor-based Client-side Watermarking in Federated Learning": {
        "title": "Harmless Backdoor-based Client-side Watermarking in Federated Learning",
        "link": "https://arxiv.org/abs/2410.21179",
        "summary": "arXiv:2410.21179v2 Announce Type: replace \nAbstract: Protecting intellectual property (IP) in federated learning (FL) is increasingly important as clients contribute proprietary data to collaboratively train models. Model watermarking, particularly through backdoor-based methods, has emerged as a popular approach for verifying ownership and contributions in deep neural networks trained via FL. By manipulating their datasets, clients can embed a secret pattern, resulting in non-intuitive predictions that serve as proof of participation, useful for claiming incentives or IP co-ownership. However, this technique faces practical challenges: (i) client watermarks can collide, leading to ambiguous ownership claims, and (ii) malicious clients may exploit watermarks to manipulate model predictions for harmful purposes. To address these issues, we propose Sanitizer, a server-side method that ensures client-embedded backdoors can only be activated in harmless environments but not natural queries. It identifies subnets within client-submitted models, extracts backdoors throughout the FL process, and confines them to harmless, client-specific input subspaces. This approach not only enhances Sanitizer's efficiency but also resolves conflicts when clients use similar triggers with different target labels. Our empirical results demonstrate that Sanitizer achieves near-perfect success verifying client contributions while mitigating the risks of malicious watermark use. Additionally, it reduces GPU memory consumption by 85% and cuts processing time by at least 5x compared to the baseline. Our code is open-sourced at https://hku-tasr.github.io/Sanitizer/.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Contextual Agent Security: A Policy for Every Purpose": {
        "title": "Contextual Agent Security: A Policy for Every Purpose",
        "link": "https://arxiv.org/abs/2501.17070",
        "summary": "arXiv:2501.17070v3 Announce Type: replace \nAbstract: Judging an action's safety requires knowledge of the context in which the action takes place. To human agents who act in various contexts, this may seem obvious: performing an action such as email deletion may or may not be appropriate depending on the email's content, the goal (e.g., to erase sensitive emails or to clean up trash), and the type of email address (e.g., work or personal). Unlike people, computational systems have often had only limited agency in limited contexts. Thus, manually crafted policies and user confirmation (e.g., smartphone app permissions or network access control lists), while imperfect, have sufficed to restrict harmful actions. However, with the upcoming deployment of generalist agents that support a multitude of tasks (e.g., an automated personal assistant), we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual agent security (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Registration, Detection, and Deregistration: Analyzing DNS Abuse for Phishing Attacks": {
        "title": "Registration, Detection, and Deregistration: Analyzing DNS Abuse for Phishing Attacks",
        "link": "https://arxiv.org/abs/2502.09549",
        "summary": "arXiv:2502.09549v2 Announce Type: replace \nAbstract: Phishing continues to pose a significant cybersecurity threat. While blocklists currently serve as a primary defense, due to their reactive, passive nature, these delayed responses leave phishing websites operational long enough to harm potential victims. It is essential to address this fundamental challenge at the root, particularly in phishing domains. Domain registration presents a crucial intervention point, as domains serve as the primary gateway between users and websites. We conduct a comprehensive longitudinal analysis of 690,502 unique phishing domains, spanning a 39 month period, to examine their characteristics and behavioral patterns throughout their lifecycle-from initial registration to detection and eventual deregistration. We find that 66.1% of the domains in our dataset are maliciously registered, leveraging cost-effective TLDs and targeting brands by mimicking their domain names under alternative TLDs (e.g., .top and .tk) instead of the TLDs under which the brand domains are registered (e.g., .com and .ru). We also observe minimal improvements in detection speed for maliciously registered domains compared to compromised domains. Detection times vary widely across blocklists, and phishing domains remain accessible for an average of 11.5 days after detection, prolonging their potential impact. Our systematic investigation uncovers key patterns from registration through detection to deregistration, which could be leveraged to enhance anti-phishing active defenses at the DNS level.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Demoting Security via Exploitation of Cache Demote Operation in Intel's Latest ISA Extension": {
        "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's Latest ISA Extension",
        "link": "https://arxiv.org/abs/2503.10074",
        "summary": "arXiv:2503.10074v2 Announce Type: replace \nAbstract: ISA extensions are increasingly adopted to boost the performance of specialized workloads without requiring an entire architectural redesign. However, these enhancements can inadvertently expose new attack surfaces in the microarchitecture. In this paper, we investigate Intel's recently introduced cldemote extension, which promotes efficient data sharing by transferring cache lines from upper-level caches to the Last Level Cache (LLC). Despite its performance benefits, we uncover critical properties-unprivileged access, inter-cache state transition, and fault suppression-that render cldemote exploitable for microarchitectural attacks. We propose two new attack primitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote constructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate of 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on Linux. Furthermore, we show that leveraging cldemote accelerates eviction set construction in non-inclusive LLC designs by obviating the need for helper threads or extensive cache conflicts, thereby reducing construction time by 36% yet retaining comparable success rates. Finally, we examine how ISA extensions contribute to broader microarchitectural attacks, identifying five key exploitable characteristics and categorizing four distinct attack types. We also discuss potential countermeasures, highlighting the far-reaching security implications of emerging ISA extensions.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Koney: A Cyber Deception Orchestration Framework for Kubernetes": {
        "title": "Koney: A Cyber Deception Orchestration Framework for Kubernetes",
        "link": "https://arxiv.org/abs/2504.02431",
        "summary": "arXiv:2504.02431v2 Announce Type: replace \nAbstract: System operators responsible for protecting software applications remain hesitant to implement cyber deception technology, including methods that place traps to catch attackers, despite its proven benefits. Overcoming their concerns removes a barrier that currently hinders industry adoption of deception technology. Our work introduces deception policy documents to describe deception technology \"as code\" and pairs them with Koney, a Kubernetes operator, which facilitates the setup, rotation, monitoring, and removal of traps in Kubernetes. We leverage cloud-native technologies, such as service meshes and eBPF, to automatically add traps to containerized software applications, without having access to the source code. We focus specifically on operational properties, such as maintainability, scalability, and simplicity, which we consider essential to accelerate the adoption of cyber deception technology and to facilitate further research on cyber deception.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Context Switching for Secure Multi-programming of Near-Term Quantum Computers": {
        "title": "Context Switching for Secure Multi-programming of Near-Term Quantum Computers",
        "link": "https://arxiv.org/abs/2504.07048",
        "summary": "arXiv:2504.07048v3 Announce Type: replace \nAbstract: Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers can exploit crosstalk without knowledge of the hardware error profile. ZKTAs can alter victim program outputs in 40% of cases on commercial systems.\n  We identify that ZKTAs succeed because the attacker's program consistently runs with the same victim program in a fixed context. To mitigate this, we propose QONTEXTS: a context-switching technique that defends against ZKTAs by running programs across multiple contexts, each handling only a subset of trials. QONTEXTS uses multi-programming with frequent context switching while identifying a unique set of programs for each context. This helps limit only a fraction of execution to ZKTAs. We enhance QONTEXTS with attack detection capabilities that compare the distributions from different contexts against each other to identify noisy contexts executed with ZKTAs. Our evaluations on real IBMQ systems show that QONTEXTS increases program resilience by three orders of magnitude and fidelity by 1.33$\\times$ on average. Moreover, QONTEXTS improves throughput by 2$\\times$, advancing security in multi-programmed environments.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization": {
        "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization",
        "link": "https://arxiv.org/abs/2504.07717",
        "summary": "arXiv:2504.07717v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "CAShift: Benchmarking Log-Based Cloud Attack Detection under Normality Shift": {
        "title": "CAShift: Benchmarking Log-Based Cloud Attack Detection under Normality Shift",
        "link": "https://arxiv.org/abs/2504.09115",
        "summary": "arXiv:2504.09115v2 Announce Type: replace \nAbstract: With the rapid advancement of cloud-native computing, securing cloud environments has become an important task. Log-based Anomaly Detection (LAD) is the most representative technique used in different systems for attack detection and safety guarantee, where multiple LAD methods and relevant datasets have been proposed. However, even though some of these datasets are specifically prepared for cloud systems, they only cover limited cloud behaviors and lack information from a whole-system perspective. Besides, another critical issue to consider is normality shift, which implies the test distribution could differ from the training distribution and highly affects the performance of LAD. Unfortunately, existing works only focus on simple shift types such as chronological changes, while other important and cloud-specific shift types are ignored, e.g., the distribution shift introduced by different deployed cloud architectures. Therefore, creating a new dataset that covers diverse behaviors of cloud systems and normality shift types is necessary.\n  To fill the gap in evaluating LAD under real-world conditions, we present CAShift, the first normality shift-aware dataset for cloud systems. CAShift captures three shift types, including application, version, and cloud architecture shifts, and includes 20 diverse attack scenarios across various cloud components. Using CAShift, we conduct an empirical study showing that (1) all LAD methods are significantly affected by normality shifts, with performance drops of up to 34%, and (2) continuous learning techniques can improve F1-scores by up to 27%, depending on data usage and algorithm choice. Based on our findings, we offer valuable implications for future research in designing more robust LAD models and methods for LAD shift adaptation.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "ControlNET: A Firewall for RAG-based LLM System": {
        "title": "ControlNET: A Firewall for RAG-based LLM System",
        "link": "https://arxiv.org/abs/2504.09593",
        "summary": "arXiv:2504.09593v2 Announce Type: replace \nAbstract: Retrieval-Augmented Generation (RAG) has significantly enhanced the factual accuracy and domain adaptability of Large Language Models (LLMs). This advancement has enabled their widespread deployment across sensitive domains such as healthcare, finance, and enterprise applications. RAG mitigates hallucinations by integrating external knowledge, yet introduces privacy risk and security risk, notably data breaching risk and data poisoning risk. While recent studies have explored prompt injection and poisoning attacks, there remains a significant gap in comprehensive research on controlling inbound and outbound query flows to mitigate these threats. In this paper, we propose an AI firewall, ControlNET, designed to safeguard RAG-based LLM systems from these vulnerabilities. ControlNET controls query flows by leveraging activation shift phenomena to detect adversarial queries and mitigate their impact through semantic divergence. We conduct comprehensive experiments on four different benchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using state-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results demonstrate that ControlNET achieves over 0.909 AUROC in detecting and mitigating security threats while preserving system harmlessness. Overall, ControlNET offers an effective, robust, harmless defense mechanism, marking a significant advancement toward the secure deployment of RAG-based LLM systems.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "Protecting Confidentiality, Privacy and Integrity in Collaborative Learning": {
        "title": "Protecting Confidentiality, Privacy and Integrity in Collaborative Learning",
        "link": "https://arxiv.org/abs/2412.08534",
        "summary": "arXiv:2412.08534v2 Announce Type: replace-cross \nAbstract: A collaboration between dataset owners and model owners is needed to facilitate effective machine learning (ML) training. During this collaboration, however, dataset owners and model owners want to protect the confidentiality of their respective assets (i.e., datasets, models and training code), with the dataset owners also caring about the privacy of individual users whose data is in their datasets. Existing solutions either provide limited confidentiality for models and training code, or suffer from privacy issues due to collusion.\n  We present Citadel++, a collaborative ML training system designed to simultaneously protect the confidentiality of datasets, models and training code as well as the privacy of individual users. Citadel++ enhances differential privacy mechanisms to safeguard the privacy of individual user data while maintaining model utility. By employing Virtual Machine-level Trusted Execution Environments (TEEs) as well as the improved sandboxing and integrity mechanisms through OS-level techniques, Citadel++ effectively preserves the confidentiality of datasets, models and training code, and enforces our privacy mechanisms even when the models and training code have been maliciously designed. Our experiments show that Citadel++ provides model utility and performance while adhering to the confidentiality and privacy requirements of dataset owners and model owners, outperforming the state-of-the-art privacy-preserving training systems by up to 543x on CPU and 113x on GPU TEEs.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    },
    "A Shapley Value Estimation Speedup for Efficient Explainable Quantum AI": {
        "title": "A Shapley Value Estimation Speedup for Efficient Explainable Quantum AI",
        "link": "https://arxiv.org/abs/2412.14639",
        "summary": "arXiv:2412.14639v2 Announce Type: replace-cross \nAbstract: This work focuses on developing efficient post-hoc explanations for quantum AI algorithms. In classical contexts, the cooperative game theory concept of the Shapley value adapts naturally to post-hoc explanations, where it can be used to identify which factors are important in an AI's decision-making process. An interesting question is how to translate Shapley values to the quantum setting and whether quantum effects could be used to accelerate their calculation. We propose quantum algorithms that can extract Shapley values within some confidence interval. Our method is capable of quadratically outperforming classical Monte Carlo approaches to approximating Shapley values up to polylogarithmic factors in various circumstances. We demonstrate the validity of our approach empirically with specific voting games and provide rigorous proofs of performance for general cooperative games.",
        "published": "Fri, 18 Apr 2025 00:00:00 -0400",
        "source": "Security arxiv"
    }
}